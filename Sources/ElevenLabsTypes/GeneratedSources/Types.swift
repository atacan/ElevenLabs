// Generated by swift-openapi-generator, do not modify.
@_spi(Generated) import OpenAPIRuntime
#if os(Linux)
@preconcurrency import struct Foundation.URL
@preconcurrency import struct Foundation.Data
@preconcurrency import struct Foundation.Date
#else
import struct Foundation.URL
import struct Foundation.Data
import struct Foundation.Date
#endif
/// A type that performs HTTP operations defined by the OpenAPI document.
public protocol APIProtocol: Sendable {
    /// Text To Speech
    ///
    /// Converts text into speech using a voice of your choice and returns audio.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/post(text_to_speech_full)`.
    func text_to_speech_full(_ input: Operations.text_to_speech_full.Input) async throws -> Operations.text_to_speech_full.Output
    /// Text To Speech With Timestamps
    ///
    /// Generate speech from text with precise character-level timing information for audio-text synchronization.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/with-timestamps`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/with-timestamps/post(text_to_speech_full_with_timestamps)`.
    func text_to_speech_full_with_timestamps(_ input: Operations.text_to_speech_full_with_timestamps.Input) async throws -> Operations.text_to_speech_full_with_timestamps.Output
    /// Text To Speech Streaming
    ///
    /// Converts text into speech using a voice of your choice and returns audio as an audio stream.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/stream`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/post(text_to_speech_stream)`.
    func text_to_speech_stream(_ input: Operations.text_to_speech_stream.Input) async throws -> Operations.text_to_speech_stream.Output
    /// Text To Speech Streaming With Timestamps
    ///
    /// Converts text into speech using a voice of your choice and returns a stream of JSONs containing audio as a base64 encoded string together with information on when which character was spoken.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/stream/with-timestamps`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/with-timestamps/post(text_to_speech_stream_with_timestamps)`.
    func text_to_speech_stream_with_timestamps(_ input: Operations.text_to_speech_stream_with_timestamps.Input) async throws -> Operations.text_to_speech_stream_with_timestamps.Output
    /// Speech To Text
    ///
    /// Transcribe an audio or video file. If webhook is set to true, the request will be processed asynchronously and results sent to configured webhooks. When use_multi_channel is true and the provided audio has multiple channels, a 'transcripts' object with separate transcripts for each channel is returned. Otherwise, returns a single transcript. The optional webhook_metadata parameter allows you to attach custom data that will be included in webhook responses for request correlation and tracking.
    ///
    /// - Remark: HTTP `POST /v1/speech-to-text`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)`.
    func speech_to_text(_ input: Operations.speech_to_text.Input) async throws -> Operations.speech_to_text.Output
    /// Get Transcript By Id
    ///
    /// Retrieve a previously generated transcript by its ID.
    ///
    /// - Remark: HTTP `GET /v1/speech-to-text/transcripts/{transcription_id}`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)`.
    func get_transcript_by_id(_ input: Operations.get_transcript_by_id.Input) async throws -> Operations.get_transcript_by_id.Output
    /// Delete Transcript By Id
    ///
    /// Delete a previously generated transcript by its ID.
    ///
    /// - Remark: HTTP `DELETE /v1/speech-to-text/transcripts/{transcription_id}`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)`.
    func delete_transcript_by_id(_ input: Operations.delete_transcript_by_id.Input) async throws -> Operations.delete_transcript_by_id.Output
}

/// Convenience overloads for operation inputs.
extension APIProtocol {
    /// Text To Speech
    ///
    /// Converts text into speech using a voice of your choice and returns audio.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/post(text_to_speech_full)`.
    public func text_to_speech_full(
        path: Operations.text_to_speech_full.Input.Path,
        query: Operations.text_to_speech_full.Input.Query = .init(),
        headers: Operations.text_to_speech_full.Input.Headers = .init(),
        body: Operations.text_to_speech_full.Input.Body
    ) async throws -> Operations.text_to_speech_full.Output {
        try await text_to_speech_full(Operations.text_to_speech_full.Input(
            path: path,
            query: query,
            headers: headers,
            body: body
        ))
    }
    /// Text To Speech With Timestamps
    ///
    /// Generate speech from text with precise character-level timing information for audio-text synchronization.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/with-timestamps`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/with-timestamps/post(text_to_speech_full_with_timestamps)`.
    public func text_to_speech_full_with_timestamps(
        path: Operations.text_to_speech_full_with_timestamps.Input.Path,
        query: Operations.text_to_speech_full_with_timestamps.Input.Query = .init(),
        headers: Operations.text_to_speech_full_with_timestamps.Input.Headers = .init(),
        body: Operations.text_to_speech_full_with_timestamps.Input.Body
    ) async throws -> Operations.text_to_speech_full_with_timestamps.Output {
        try await text_to_speech_full_with_timestamps(Operations.text_to_speech_full_with_timestamps.Input(
            path: path,
            query: query,
            headers: headers,
            body: body
        ))
    }
    /// Text To Speech Streaming
    ///
    /// Converts text into speech using a voice of your choice and returns audio as an audio stream.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/stream`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/post(text_to_speech_stream)`.
    public func text_to_speech_stream(
        path: Operations.text_to_speech_stream.Input.Path,
        query: Operations.text_to_speech_stream.Input.Query = .init(),
        headers: Operations.text_to_speech_stream.Input.Headers = .init(),
        body: Operations.text_to_speech_stream.Input.Body
    ) async throws -> Operations.text_to_speech_stream.Output {
        try await text_to_speech_stream(Operations.text_to_speech_stream.Input(
            path: path,
            query: query,
            headers: headers,
            body: body
        ))
    }
    /// Text To Speech Streaming With Timestamps
    ///
    /// Converts text into speech using a voice of your choice and returns a stream of JSONs containing audio as a base64 encoded string together with information on when which character was spoken.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/stream/with-timestamps`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/with-timestamps/post(text_to_speech_stream_with_timestamps)`.
    public func text_to_speech_stream_with_timestamps(
        path: Operations.text_to_speech_stream_with_timestamps.Input.Path,
        query: Operations.text_to_speech_stream_with_timestamps.Input.Query = .init(),
        headers: Operations.text_to_speech_stream_with_timestamps.Input.Headers = .init(),
        body: Operations.text_to_speech_stream_with_timestamps.Input.Body
    ) async throws -> Operations.text_to_speech_stream_with_timestamps.Output {
        try await text_to_speech_stream_with_timestamps(Operations.text_to_speech_stream_with_timestamps.Input(
            path: path,
            query: query,
            headers: headers,
            body: body
        ))
    }
    /// Speech To Text
    ///
    /// Transcribe an audio or video file. If webhook is set to true, the request will be processed asynchronously and results sent to configured webhooks. When use_multi_channel is true and the provided audio has multiple channels, a 'transcripts' object with separate transcripts for each channel is returned. Otherwise, returns a single transcript. The optional webhook_metadata parameter allows you to attach custom data that will be included in webhook responses for request correlation and tracking.
    ///
    /// - Remark: HTTP `POST /v1/speech-to-text`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)`.
    public func speech_to_text(
        query: Operations.speech_to_text.Input.Query = .init(),
        headers: Operations.speech_to_text.Input.Headers = .init(),
        body: Operations.speech_to_text.Input.Body
    ) async throws -> Operations.speech_to_text.Output {
        try await speech_to_text(Operations.speech_to_text.Input(
            query: query,
            headers: headers,
            body: body
        ))
    }
    /// Get Transcript By Id
    ///
    /// Retrieve a previously generated transcript by its ID.
    ///
    /// - Remark: HTTP `GET /v1/speech-to-text/transcripts/{transcription_id}`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)`.
    public func get_transcript_by_id(
        path: Operations.get_transcript_by_id.Input.Path,
        headers: Operations.get_transcript_by_id.Input.Headers = .init()
    ) async throws -> Operations.get_transcript_by_id.Output {
        try await get_transcript_by_id(Operations.get_transcript_by_id.Input(
            path: path,
            headers: headers
        ))
    }
    /// Delete Transcript By Id
    ///
    /// Delete a previously generated transcript by its ID.
    ///
    /// - Remark: HTTP `DELETE /v1/speech-to-text/transcripts/{transcription_id}`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)`.
    public func delete_transcript_by_id(
        path: Operations.delete_transcript_by_id.Input.Path,
        headers: Operations.delete_transcript_by_id.Input.Headers = .init()
    ) async throws -> Operations.delete_transcript_by_id.Output {
        try await delete_transcript_by_id(Operations.delete_transcript_by_id.Input(
            path: path,
            headers: headers
        ))
    }
}

/// Server URLs defined in the OpenAPI document.
public enum Servers {}

/// Types generated from the components section of the OpenAPI document.
public enum Components {
    /// Types generated from the `#/components/schemas` section of the OpenAPI document.
    public enum Schemas {
        /// - Remark: Generated from `#/components/schemas/AdditionalFormatResponseModel`.
        public struct AdditionalFormatResponseModel: Codable, Hashable, Sendable {
            /// The requested format.
            ///
            /// - Remark: Generated from `#/components/schemas/AdditionalFormatResponseModel/requested_format`.
            public var requested_format: Swift.String
            /// The file extension of the additional format.
            ///
            /// - Remark: Generated from `#/components/schemas/AdditionalFormatResponseModel/file_extension`.
            public var file_extension: Swift.String
            /// The content type of the additional format.
            ///
            /// - Remark: Generated from `#/components/schemas/AdditionalFormatResponseModel/content_type`.
            public var content_type: Swift.String
            /// Whether the content is base64 encoded.
            ///
            /// - Remark: Generated from `#/components/schemas/AdditionalFormatResponseModel/is_base64_encoded`.
            public var is_base64_encoded: Swift.Bool
            /// The content of the additional format.
            ///
            /// - Remark: Generated from `#/components/schemas/AdditionalFormatResponseModel/content`.
            public var content: Swift.String
            /// Creates a new `AdditionalFormatResponseModel`.
            ///
            /// - Parameters:
            ///   - requested_format: The requested format.
            ///   - file_extension: The file extension of the additional format.
            ///   - content_type: The content type of the additional format.
            ///   - is_base64_encoded: Whether the content is base64 encoded.
            ///   - content: The content of the additional format.
            public init(
                requested_format: Swift.String,
                file_extension: Swift.String,
                content_type: Swift.String,
                is_base64_encoded: Swift.Bool,
                content: Swift.String
            ) {
                self.requested_format = requested_format
                self.file_extension = file_extension
                self.content_type = content_type
                self.is_base64_encoded = is_base64_encoded
                self.content = content
            }
            public enum CodingKeys: String, CodingKey {
                case requested_format
                case file_extension
                case content_type
                case is_base64_encoded
                case content
            }
        }
        /// - Remark: Generated from `#/components/schemas/AudioWithTimestampsResponseModel`.
        public struct AudioWithTimestampsResponseModel: Codable, Hashable, Sendable {
            /// Base64 encoded audio data
            ///
            /// - Remark: Generated from `#/components/schemas/AudioWithTimestampsResponseModel/audio_base64`.
            public var audio_base64: Swift.String
            /// Timestamp information for each character in the original text
            ///
            /// - Remark: Generated from `#/components/schemas/AudioWithTimestampsResponseModel/alignment`.
            public var alignment: Components.Schemas.CharacterAlignmentResponseModel?
            /// Timestamp information for each character in the normalized text
            ///
            /// - Remark: Generated from `#/components/schemas/AudioWithTimestampsResponseModel/normalized_alignment`.
            public var normalized_alignment: Components.Schemas.CharacterAlignmentResponseModel?
            /// Creates a new `AudioWithTimestampsResponseModel`.
            ///
            /// - Parameters:
            ///   - audio_base64: Base64 encoded audio data
            ///   - alignment: Timestamp information for each character in the original text
            ///   - normalized_alignment: Timestamp information for each character in the normalized text
            public init(
                audio_base64: Swift.String,
                alignment: Components.Schemas.CharacterAlignmentResponseModel? = nil,
                normalized_alignment: Components.Schemas.CharacterAlignmentResponseModel? = nil
            ) {
                self.audio_base64 = audio_base64
                self.alignment = alignment
                self.normalized_alignment = normalized_alignment
            }
            public enum CodingKeys: String, CodingKey {
                case audio_base64
                case alignment
                case normalized_alignment
            }
        }
        /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post`.
        @frozen public enum Body_Speech_to_Text_v1_speech_to_text_post: Sendable, Hashable {
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/model_id`.
            public struct model_idPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `model_idPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case model_id(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.model_idPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/file`.
            public struct filePayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `filePayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case file(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.filePayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/language_code`.
            public struct language_codePayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `language_codePayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case language_code(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.language_codePayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/tag_audio_events`.
            public struct tag_audio_eventsPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `tag_audio_eventsPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case tag_audio_events(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.tag_audio_eventsPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/num_speakers`.
            public struct num_speakersPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `num_speakersPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case num_speakers(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.num_speakersPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/timestamps_granularity`.
            public struct timestamps_granularityPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `timestamps_granularityPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case timestamps_granularity(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.timestamps_granularityPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/diarize`.
            public struct diarizePayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `diarizePayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case diarize(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.diarizePayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/diarization_threshold`.
            public struct diarization_thresholdPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `diarization_thresholdPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case diarization_threshold(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.diarization_thresholdPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/additional_formats`.
            public struct additional_formatsPayload: Sendable, Hashable {
                public var body: Components.Schemas.ExportOptions
                /// Creates a new `additional_formatsPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: Components.Schemas.ExportOptions) {
                    self.body = body
                }
            }
            case additional_formats(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.additional_formatsPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/file_format`.
            public struct file_formatPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `file_formatPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case file_format(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.file_formatPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/cloud_storage_url`.
            public struct cloud_storage_urlPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `cloud_storage_urlPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case cloud_storage_url(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.cloud_storage_urlPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/webhook`.
            public struct webhookPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `webhookPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case webhook(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.webhookPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/webhook_id`.
            public struct webhook_idPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `webhook_idPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case webhook_id(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.webhook_idPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/temperature`.
            public struct temperaturePayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `temperaturePayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case temperature(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.temperaturePayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/seed`.
            public struct seedPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `seedPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case seed(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.seedPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/use_multi_channel`.
            public struct use_multi_channelPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `use_multi_channelPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case use_multi_channel(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.use_multi_channelPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/webhook_metadata`.
            public struct webhook_metadataPayload: Sendable, Hashable {
                /// Optional metadata to be included in the webhook response. This should be a JSON string representing an object with a maximum depth of 2 levels and maximum size of 16KB. Useful for tracking internal IDs, job references, or other contextual information.
                ///
                /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/webhook_metadata/content/body`.
                public struct bodyPayload: Codable, Hashable, Sendable {
                    /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/webhook_metadata/content/body/value1`.
                    public var value1: Swift.String?
                    /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/webhook_metadata/content/body/value2`.
                    public struct Value2Payload: Codable, Hashable, Sendable {
                        /// A container of undocumented properties.
                        public var additionalProperties: OpenAPIRuntime.OpenAPIObjectContainer
                        /// Creates a new `Value2Payload`.
                        ///
                        /// - Parameters:
                        ///   - additionalProperties: A container of undocumented properties.
                        public init(additionalProperties: OpenAPIRuntime.OpenAPIObjectContainer = .init()) {
                            self.additionalProperties = additionalProperties
                        }
                        public init(from decoder: any Decoder) throws {
                            additionalProperties = try decoder.decodeAdditionalProperties(knownKeys: [])
                        }
                        public func encode(to encoder: any Encoder) throws {
                            try encoder.encodeAdditionalProperties(additionalProperties)
                        }
                    }
                    /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/webhook_metadata/content/body/value2`.
                    public var value2: Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.webhook_metadataPayload.bodyPayload.Value2Payload?
                    /// Creates a new `bodyPayload`.
                    ///
                    /// - Parameters:
                    ///   - value1:
                    ///   - value2:
                    public init(
                        value1: Swift.String? = nil,
                        value2: Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.webhook_metadataPayload.bodyPayload.Value2Payload? = nil
                    ) {
                        self.value1 = value1
                        self.value2 = value2
                    }
                    public init(from decoder: any Decoder) throws {
                        var errors: [any Error] = []
                        do {
                            self.value1 = try decoder.decodeFromSingleValueContainer()
                        } catch {
                            errors.append(error)
                        }
                        do {
                            self.value2 = try .init(from: decoder)
                        } catch {
                            errors.append(error)
                        }
                        try Swift.DecodingError.verifyAtLeastOneSchemaIsNotNil(
                            [
                                self.value1,
                                self.value2
                            ],
                            type: Self.self,
                            codingPath: decoder.codingPath,
                            errors: errors
                        )
                    }
                    public func encode(to encoder: any Encoder) throws {
                        try encoder.encodeFirstNonNilValueToSingleValueContainer([
                            self.value1
                        ])
                        try self.value2?.encode(to: encoder)
                    }
                }
                public var body: Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.webhook_metadataPayload.bodyPayload
                /// Creates a new `webhook_metadataPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.webhook_metadataPayload.bodyPayload) {
                    self.body = body
                }
            }
            case webhook_metadata(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.webhook_metadataPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/entity_detection`.
            public struct entity_detectionPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `entity_detectionPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case entity_detection(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.entity_detectionPayload>)
            /// - Remark: Generated from `#/components/schemas/Body_Speech_to_Text_v1_speech_to_text_post/keyterms`.
            public struct keytermsPayload: Sendable, Hashable {
                public var body: OpenAPIRuntime.HTTPBody
                /// Creates a new `keytermsPayload`.
                ///
                /// - Parameters:
                ///   - body:
                public init(body: OpenAPIRuntime.HTTPBody) {
                    self.body = body
                }
            }
            case keyterms(OpenAPIRuntime.MultipartPart<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post.keytermsPayload>)
            case undocumented(OpenAPIRuntime.MultipartRawPart)
        }
        /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full`.
        public struct Body_text_to_speech_full: Codable, Hashable, Sendable {
            /// The text that will get converted into speech.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/text`.
            public var text: Swift.String
            /// Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/model_id`.
            public var model_id: Swift.String?
            /// Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/language_code`.
            public var language_code: Swift.String?
            /// Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/voice_settings`.
            public var voice_settings: Components.Schemas.VoiceSettingsResponseModel?
            /// A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/pronunciation_dictionary_locators`.
            public var pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]?
            /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/seed`.
            public var seed: Swift.Int?
            /// The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/previous_text`.
            public var previous_text: Swift.String?
            /// The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/next_text`.
            public var next_text: Swift.String?
            /// A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/previous_request_ids`.
            public var previous_request_ids: [Swift.String]?
            /// A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/next_request_ids`.
            public var next_request_ids: [Swift.String]?
            /// If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/use_pvc_as_ivc`.
            @available(*, deprecated)
            public var use_pvc_as_ivc: Swift.Bool?
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/apply_text_normalization`.
            @frozen public enum apply_text_normalizationPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case auto = "auto"
                case on = "on"
                case off = "off"
            }
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/apply_text_normalization`.
            public var apply_text_normalization: Components.Schemas.Body_text_to_speech_full.apply_text_normalizationPayload?
            /// This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full/apply_language_text_normalization`.
            public var apply_language_text_normalization: Swift.Bool?
            /// Creates a new `Body_text_to_speech_full`.
            ///
            /// - Parameters:
            ///   - text: The text that will get converted into speech.
            ///   - model_id: Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///   - language_code: Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///   - voice_settings: Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///   - pronunciation_dictionary_locators: A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///   - seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///   - previous_text: The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - next_text: The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - previous_request_ids: A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - next_request_ids: A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - use_pvc_as_ivc: If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///   - apply_text_normalization: This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///   - apply_language_text_normalization: This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            public init(
                text: Swift.String,
                model_id: Swift.String? = nil,
                language_code: Swift.String? = nil,
                voice_settings: Components.Schemas.VoiceSettingsResponseModel? = nil,
                pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]? = nil,
                seed: Swift.Int? = nil,
                previous_text: Swift.String? = nil,
                next_text: Swift.String? = nil,
                previous_request_ids: [Swift.String]? = nil,
                next_request_ids: [Swift.String]? = nil,
                use_pvc_as_ivc: Swift.Bool? = nil,
                apply_text_normalization: Components.Schemas.Body_text_to_speech_full.apply_text_normalizationPayload? = nil,
                apply_language_text_normalization: Swift.Bool? = nil
            ) {
                self.text = text
                self.model_id = model_id
                self.language_code = language_code
                self.voice_settings = voice_settings
                self.pronunciation_dictionary_locators = pronunciation_dictionary_locators
                self.seed = seed
                self.previous_text = previous_text
                self.next_text = next_text
                self.previous_request_ids = previous_request_ids
                self.next_request_ids = next_request_ids
                self.use_pvc_as_ivc = use_pvc_as_ivc
                self.apply_text_normalization = apply_text_normalization
                self.apply_language_text_normalization = apply_language_text_normalization
            }
            public enum CodingKeys: String, CodingKey {
                case text
                case model_id
                case language_code
                case voice_settings
                case pronunciation_dictionary_locators
                case seed
                case previous_text
                case next_text
                case previous_request_ids
                case next_request_ids
                case use_pvc_as_ivc
                case apply_text_normalization
                case apply_language_text_normalization
            }
        }
        /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps`.
        public struct Body_text_to_speech_full_with_timestamps: Codable, Hashable, Sendable {
            /// The text that will get converted into speech.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/text`.
            public var text: Swift.String
            /// Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/model_id`.
            public var model_id: Swift.String?
            /// Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/language_code`.
            public var language_code: Swift.String?
            /// Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/voice_settings`.
            public var voice_settings: Components.Schemas.VoiceSettingsResponseModel?
            /// A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/pronunciation_dictionary_locators`.
            public var pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]?
            /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/seed`.
            public var seed: Swift.Int?
            /// The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/previous_text`.
            public var previous_text: Swift.String?
            /// The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/next_text`.
            public var next_text: Swift.String?
            /// A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/previous_request_ids`.
            public var previous_request_ids: [Swift.String]?
            /// A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/next_request_ids`.
            public var next_request_ids: [Swift.String]?
            /// If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/use_pvc_as_ivc`.
            @available(*, deprecated)
            public var use_pvc_as_ivc: Swift.Bool?
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/apply_text_normalization`.
            @frozen public enum apply_text_normalizationPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case auto = "auto"
                case on = "on"
                case off = "off"
            }
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/apply_text_normalization`.
            public var apply_text_normalization: Components.Schemas.Body_text_to_speech_full_with_timestamps.apply_text_normalizationPayload?
            /// This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_full_with_timestamps/apply_language_text_normalization`.
            public var apply_language_text_normalization: Swift.Bool?
            /// Creates a new `Body_text_to_speech_full_with_timestamps`.
            ///
            /// - Parameters:
            ///   - text: The text that will get converted into speech.
            ///   - model_id: Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///   - language_code: Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///   - voice_settings: Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///   - pronunciation_dictionary_locators: A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///   - seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///   - previous_text: The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - next_text: The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - previous_request_ids: A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - next_request_ids: A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - use_pvc_as_ivc: If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///   - apply_text_normalization: This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///   - apply_language_text_normalization: This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            public init(
                text: Swift.String,
                model_id: Swift.String? = nil,
                language_code: Swift.String? = nil,
                voice_settings: Components.Schemas.VoiceSettingsResponseModel? = nil,
                pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]? = nil,
                seed: Swift.Int? = nil,
                previous_text: Swift.String? = nil,
                next_text: Swift.String? = nil,
                previous_request_ids: [Swift.String]? = nil,
                next_request_ids: [Swift.String]? = nil,
                use_pvc_as_ivc: Swift.Bool? = nil,
                apply_text_normalization: Components.Schemas.Body_text_to_speech_full_with_timestamps.apply_text_normalizationPayload? = nil,
                apply_language_text_normalization: Swift.Bool? = nil
            ) {
                self.text = text
                self.model_id = model_id
                self.language_code = language_code
                self.voice_settings = voice_settings
                self.pronunciation_dictionary_locators = pronunciation_dictionary_locators
                self.seed = seed
                self.previous_text = previous_text
                self.next_text = next_text
                self.previous_request_ids = previous_request_ids
                self.next_request_ids = next_request_ids
                self.use_pvc_as_ivc = use_pvc_as_ivc
                self.apply_text_normalization = apply_text_normalization
                self.apply_language_text_normalization = apply_language_text_normalization
            }
            public enum CodingKeys: String, CodingKey {
                case text
                case model_id
                case language_code
                case voice_settings
                case pronunciation_dictionary_locators
                case seed
                case previous_text
                case next_text
                case previous_request_ids
                case next_request_ids
                case use_pvc_as_ivc
                case apply_text_normalization
                case apply_language_text_normalization
            }
        }
        /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream`.
        public struct Body_text_to_speech_stream: Codable, Hashable, Sendable {
            /// The text that will get converted into speech.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/text`.
            public var text: Swift.String
            /// Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/model_id`.
            public var model_id: Swift.String?
            /// Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/language_code`.
            public var language_code: Swift.String?
            /// Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/voice_settings`.
            public var voice_settings: Components.Schemas.VoiceSettingsResponseModel?
            /// A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/pronunciation_dictionary_locators`.
            public var pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]?
            /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/seed`.
            public var seed: Swift.Int?
            /// The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/previous_text`.
            public var previous_text: Swift.String?
            /// The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/next_text`.
            public var next_text: Swift.String?
            /// A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/previous_request_ids`.
            public var previous_request_ids: [Swift.String]?
            /// A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/next_request_ids`.
            public var next_request_ids: [Swift.String]?
            /// If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/use_pvc_as_ivc`.
            @available(*, deprecated)
            public var use_pvc_as_ivc: Swift.Bool?
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/apply_text_normalization`.
            @frozen public enum apply_text_normalizationPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case auto = "auto"
                case on = "on"
                case off = "off"
            }
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/apply_text_normalization`.
            public var apply_text_normalization: Components.Schemas.Body_text_to_speech_stream.apply_text_normalizationPayload?
            /// This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream/apply_language_text_normalization`.
            public var apply_language_text_normalization: Swift.Bool?
            /// Creates a new `Body_text_to_speech_stream`.
            ///
            /// - Parameters:
            ///   - text: The text that will get converted into speech.
            ///   - model_id: Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///   - language_code: Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///   - voice_settings: Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///   - pronunciation_dictionary_locators: A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///   - seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///   - previous_text: The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - next_text: The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - previous_request_ids: A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - next_request_ids: A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - use_pvc_as_ivc: If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///   - apply_text_normalization: This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///   - apply_language_text_normalization: This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            public init(
                text: Swift.String,
                model_id: Swift.String? = nil,
                language_code: Swift.String? = nil,
                voice_settings: Components.Schemas.VoiceSettingsResponseModel? = nil,
                pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]? = nil,
                seed: Swift.Int? = nil,
                previous_text: Swift.String? = nil,
                next_text: Swift.String? = nil,
                previous_request_ids: [Swift.String]? = nil,
                next_request_ids: [Swift.String]? = nil,
                use_pvc_as_ivc: Swift.Bool? = nil,
                apply_text_normalization: Components.Schemas.Body_text_to_speech_stream.apply_text_normalizationPayload? = nil,
                apply_language_text_normalization: Swift.Bool? = nil
            ) {
                self.text = text
                self.model_id = model_id
                self.language_code = language_code
                self.voice_settings = voice_settings
                self.pronunciation_dictionary_locators = pronunciation_dictionary_locators
                self.seed = seed
                self.previous_text = previous_text
                self.next_text = next_text
                self.previous_request_ids = previous_request_ids
                self.next_request_ids = next_request_ids
                self.use_pvc_as_ivc = use_pvc_as_ivc
                self.apply_text_normalization = apply_text_normalization
                self.apply_language_text_normalization = apply_language_text_normalization
            }
            public enum CodingKeys: String, CodingKey {
                case text
                case model_id
                case language_code
                case voice_settings
                case pronunciation_dictionary_locators
                case seed
                case previous_text
                case next_text
                case previous_request_ids
                case next_request_ids
                case use_pvc_as_ivc
                case apply_text_normalization
                case apply_language_text_normalization
            }
        }
        /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps`.
        public struct Body_text_to_speech_stream_with_timestamps: Codable, Hashable, Sendable {
            /// The text that will get converted into speech.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/text`.
            public var text: Swift.String
            /// Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/model_id`.
            public var model_id: Swift.String?
            /// Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/language_code`.
            public var language_code: Swift.String?
            /// Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/voice_settings`.
            public var voice_settings: Components.Schemas.VoiceSettingsResponseModel?
            /// A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/pronunciation_dictionary_locators`.
            public var pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]?
            /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/seed`.
            public var seed: Swift.Int?
            /// The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/previous_text`.
            public var previous_text: Swift.String?
            /// The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/next_text`.
            public var next_text: Swift.String?
            /// A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/previous_request_ids`.
            public var previous_request_ids: [Swift.String]?
            /// A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/next_request_ids`.
            public var next_request_ids: [Swift.String]?
            /// If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/use_pvc_as_ivc`.
            @available(*, deprecated)
            public var use_pvc_as_ivc: Swift.Bool?
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/apply_text_normalization`.
            @frozen public enum apply_text_normalizationPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case auto = "auto"
                case on = "on"
                case off = "off"
            }
            /// This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/apply_text_normalization`.
            public var apply_text_normalization: Components.Schemas.Body_text_to_speech_stream_with_timestamps.apply_text_normalizationPayload?
            /// This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            ///
            /// - Remark: Generated from `#/components/schemas/Body_text_to_speech_stream_with_timestamps/apply_language_text_normalization`.
            public var apply_language_text_normalization: Swift.Bool?
            /// Creates a new `Body_text_to_speech_stream_with_timestamps`.
            ///
            /// - Parameters:
            ///   - text: The text that will get converted into speech.
            ///   - model_id: Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
            ///   - language_code: Language code (ISO 639-1) used to enforce a language for the model and text normalization. If the model does not support provided language code, an error will be returned.
            ///   - voice_settings: Voice settings overriding stored settings for the given voice. They are applied only on the given request.
            ///   - pronunciation_dictionary_locators: A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
            ///   - seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
            ///   - previous_text: The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - next_text: The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
            ///   - previous_request_ids: A list of request_id of the samples that were generated before this generation. Can be used to improve the speech's continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - next_request_ids: A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech's continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
            ///   - use_pvc_as_ivc: If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
            ///   - apply_text_normalization: This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
            ///   - apply_language_text_normalization: This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
            public init(
                text: Swift.String,
                model_id: Swift.String? = nil,
                language_code: Swift.String? = nil,
                voice_settings: Components.Schemas.VoiceSettingsResponseModel? = nil,
                pronunciation_dictionary_locators: [Components.Schemas.PronunciationDictionaryVersionLocatorRequestModel]? = nil,
                seed: Swift.Int? = nil,
                previous_text: Swift.String? = nil,
                next_text: Swift.String? = nil,
                previous_request_ids: [Swift.String]? = nil,
                next_request_ids: [Swift.String]? = nil,
                use_pvc_as_ivc: Swift.Bool? = nil,
                apply_text_normalization: Components.Schemas.Body_text_to_speech_stream_with_timestamps.apply_text_normalizationPayload? = nil,
                apply_language_text_normalization: Swift.Bool? = nil
            ) {
                self.text = text
                self.model_id = model_id
                self.language_code = language_code
                self.voice_settings = voice_settings
                self.pronunciation_dictionary_locators = pronunciation_dictionary_locators
                self.seed = seed
                self.previous_text = previous_text
                self.next_text = next_text
                self.previous_request_ids = previous_request_ids
                self.next_request_ids = next_request_ids
                self.use_pvc_as_ivc = use_pvc_as_ivc
                self.apply_text_normalization = apply_text_normalization
                self.apply_language_text_normalization = apply_language_text_normalization
            }
            public enum CodingKeys: String, CodingKey {
                case text
                case model_id
                case language_code
                case voice_settings
                case pronunciation_dictionary_locators
                case seed
                case previous_text
                case next_text
                case previous_request_ids
                case next_request_ids
                case use_pvc_as_ivc
                case apply_text_normalization
                case apply_language_text_normalization
            }
        }
        /// - Remark: Generated from `#/components/schemas/CharacterAlignmentResponseModel`.
        public struct CharacterAlignmentResponseModel: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/CharacterAlignmentResponseModel/characters`.
            public var characters: [Swift.String]
            /// - Remark: Generated from `#/components/schemas/CharacterAlignmentResponseModel/character_start_times_seconds`.
            public var character_start_times_seconds: [Swift.Double]
            /// - Remark: Generated from `#/components/schemas/CharacterAlignmentResponseModel/character_end_times_seconds`.
            public var character_end_times_seconds: [Swift.Double]
            /// Creates a new `CharacterAlignmentResponseModel`.
            ///
            /// - Parameters:
            ///   - characters:
            ///   - character_start_times_seconds:
            ///   - character_end_times_seconds:
            public init(
                characters: [Swift.String],
                character_start_times_seconds: [Swift.Double],
                character_end_times_seconds: [Swift.Double]
            ) {
                self.characters = characters
                self.character_start_times_seconds = character_start_times_seconds
                self.character_end_times_seconds = character_end_times_seconds
            }
            public enum CodingKeys: String, CodingKey {
                case characters
                case character_start_times_seconds
                case character_end_times_seconds
            }
        }
        /// - Remark: Generated from `#/components/schemas/DetectedEntity`.
        public struct DetectedEntity: Codable, Hashable, Sendable {
            /// The text that was identified as an entity.
            ///
            /// - Remark: Generated from `#/components/schemas/DetectedEntity/text`.
            public var text: Swift.String
            /// The type of entity detected (e.g., 'credit_card', 'email_address', 'person_name').
            ///
            /// - Remark: Generated from `#/components/schemas/DetectedEntity/entity_type`.
            public var entity_type: Swift.String
            /// Start character position in the transcript text.
            ///
            /// - Remark: Generated from `#/components/schemas/DetectedEntity/start_char`.
            public var start_char: Swift.Int
            /// End character position in the transcript text.
            ///
            /// - Remark: Generated from `#/components/schemas/DetectedEntity/end_char`.
            public var end_char: Swift.Int
            /// Creates a new `DetectedEntity`.
            ///
            /// - Parameters:
            ///   - text: The text that was identified as an entity.
            ///   - entity_type: The type of entity detected (e.g., 'credit_card', 'email_address', 'person_name').
            ///   - start_char: Start character position in the transcript text.
            ///   - end_char: End character position in the transcript text.
            public init(
                text: Swift.String,
                entity_type: Swift.String,
                start_char: Swift.Int,
                end_char: Swift.Int
            ) {
                self.text = text
                self.entity_type = entity_type
                self.start_char = start_char
                self.end_char = end_char
            }
            public enum CodingKeys: String, CodingKey {
                case text
                case entity_type
                case start_char
                case end_char
            }
        }
        /// - Remark: Generated from `#/components/schemas/DocxExportOptions`.
        public struct DocxExportOptions: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/DocxExportOptions/include_speakers`.
            public var include_speakers: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/DocxExportOptions/include_timestamps`.
            public var include_timestamps: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/DocxExportOptions/format`.
            @frozen public enum formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case docx = "docx"
            }
            /// - Remark: Generated from `#/components/schemas/DocxExportOptions/format`.
            public var format: Components.Schemas.DocxExportOptions.formatPayload
            /// - Remark: Generated from `#/components/schemas/DocxExportOptions/segment_on_silence_longer_than_s`.
            public var segment_on_silence_longer_than_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/DocxExportOptions/max_segment_duration_s`.
            public var max_segment_duration_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/DocxExportOptions/max_segment_chars`.
            public var max_segment_chars: Swift.Int?
            /// Creates a new `DocxExportOptions`.
            ///
            /// - Parameters:
            ///   - include_speakers:
            ///   - include_timestamps:
            ///   - format:
            ///   - segment_on_silence_longer_than_s:
            ///   - max_segment_duration_s:
            ///   - max_segment_chars:
            public init(
                include_speakers: Swift.Bool? = nil,
                include_timestamps: Swift.Bool? = nil,
                format: Components.Schemas.DocxExportOptions.formatPayload,
                segment_on_silence_longer_than_s: Swift.Double? = nil,
                max_segment_duration_s: Swift.Double? = nil,
                max_segment_chars: Swift.Int? = nil
            ) {
                self.include_speakers = include_speakers
                self.include_timestamps = include_timestamps
                self.format = format
                self.segment_on_silence_longer_than_s = segment_on_silence_longer_than_s
                self.max_segment_duration_s = max_segment_duration_s
                self.max_segment_chars = max_segment_chars
            }
            public enum CodingKeys: String, CodingKey {
                case include_speakers
                case include_timestamps
                case format
                case segment_on_silence_longer_than_s
                case max_segment_duration_s
                case max_segment_chars
            }
        }
        /// - Remark: Generated from `#/components/schemas/ExportOptions`.
        @frozen public enum ExportOptions: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/ExportOptions/DocxExportOptions`.
            case docx(Components.Schemas.DocxExportOptions)
            /// - Remark: Generated from `#/components/schemas/ExportOptions/HtmlExportOptions`.
            case html(Components.Schemas.HtmlExportOptions)
            /// - Remark: Generated from `#/components/schemas/ExportOptions/PdfExportOptions`.
            case pdf(Components.Schemas.PdfExportOptions)
            /// - Remark: Generated from `#/components/schemas/ExportOptions/SegmentedJsonExportOptions`.
            case segmented_json(Components.Schemas.SegmentedJsonExportOptions)
            /// - Remark: Generated from `#/components/schemas/ExportOptions/SrtExportOptions`.
            case srt(Components.Schemas.SrtExportOptions)
            /// - Remark: Generated from `#/components/schemas/ExportOptions/TxtExportOptions`.
            case txt(Components.Schemas.TxtExportOptions)
            public enum CodingKeys: String, CodingKey {
                case format
            }
            public init(from decoder: any Decoder) throws {
                let container = try decoder.container(keyedBy: CodingKeys.self)
                let discriminator = try container.decode(
                    Swift.String.self,
                    forKey: .format
                )
                switch discriminator {
                case "docx":
                    self = .docx(try .init(from: decoder))
                case "html":
                    self = .html(try .init(from: decoder))
                case "pdf":
                    self = .pdf(try .init(from: decoder))
                case "segmented_json":
                    self = .segmented_json(try .init(from: decoder))
                case "srt":
                    self = .srt(try .init(from: decoder))
                case "txt":
                    self = .txt(try .init(from: decoder))
                default:
                    throw Swift.DecodingError.unknownOneOfDiscriminator(
                        discriminatorKey: CodingKeys.format,
                        discriminatorValue: discriminator,
                        codingPath: decoder.codingPath
                    )
                }
            }
            public func encode(to encoder: any Encoder) throws {
                switch self {
                case let .docx(value):
                    try value.encode(to: encoder)
                case let .html(value):
                    try value.encode(to: encoder)
                case let .pdf(value):
                    try value.encode(to: encoder)
                case let .segmented_json(value):
                    try value.encode(to: encoder)
                case let .srt(value):
                    try value.encode(to: encoder)
                case let .txt(value):
                    try value.encode(to: encoder)
                }
            }
        }
        /// - Remark: Generated from `#/components/schemas/HTTPValidationError`.
        public struct HTTPValidationError: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/HTTPValidationError/detail`.
            public var detail: [Components.Schemas.ValidationError]?
            /// Creates a new `HTTPValidationError`.
            ///
            /// - Parameters:
            ///   - detail:
            public init(detail: [Components.Schemas.ValidationError]? = nil) {
                self.detail = detail
            }
            public enum CodingKeys: String, CodingKey {
                case detail
            }
        }
        /// - Remark: Generated from `#/components/schemas/HtmlExportOptions`.
        public struct HtmlExportOptions: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/HtmlExportOptions/include_speakers`.
            public var include_speakers: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/HtmlExportOptions/include_timestamps`.
            public var include_timestamps: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/HtmlExportOptions/format`.
            @frozen public enum formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case html = "html"
            }
            /// - Remark: Generated from `#/components/schemas/HtmlExportOptions/format`.
            public var format: Components.Schemas.HtmlExportOptions.formatPayload
            /// - Remark: Generated from `#/components/schemas/HtmlExportOptions/segment_on_silence_longer_than_s`.
            public var segment_on_silence_longer_than_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/HtmlExportOptions/max_segment_duration_s`.
            public var max_segment_duration_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/HtmlExportOptions/max_segment_chars`.
            public var max_segment_chars: Swift.Int?
            /// Creates a new `HtmlExportOptions`.
            ///
            /// - Parameters:
            ///   - include_speakers:
            ///   - include_timestamps:
            ///   - format:
            ///   - segment_on_silence_longer_than_s:
            ///   - max_segment_duration_s:
            ///   - max_segment_chars:
            public init(
                include_speakers: Swift.Bool? = nil,
                include_timestamps: Swift.Bool? = nil,
                format: Components.Schemas.HtmlExportOptions.formatPayload,
                segment_on_silence_longer_than_s: Swift.Double? = nil,
                max_segment_duration_s: Swift.Double? = nil,
                max_segment_chars: Swift.Int? = nil
            ) {
                self.include_speakers = include_speakers
                self.include_timestamps = include_timestamps
                self.format = format
                self.segment_on_silence_longer_than_s = segment_on_silence_longer_than_s
                self.max_segment_duration_s = max_segment_duration_s
                self.max_segment_chars = max_segment_chars
            }
            public enum CodingKeys: String, CodingKey {
                case include_speakers
                case include_timestamps
                case format
                case segment_on_silence_longer_than_s
                case max_segment_duration_s
                case max_segment_chars
            }
        }
        /// Response model for multichannel speech-to-text transcription.
        ///
        /// - Remark: Generated from `#/components/schemas/MultichannelSpeechToTextResponseModel`.
        public struct MultichannelSpeechToTextResponseModel: Codable, Hashable, Sendable {
            /// List of transcripts, one for each audio channel. Each transcript contains the text and word-level details for its respective channel.
            ///
            /// - Remark: Generated from `#/components/schemas/MultichannelSpeechToTextResponseModel/transcripts`.
            public var transcripts: [Components.Schemas.SpeechToTextChunkResponseModel]
            /// The transcription ID of the response.
            ///
            /// - Remark: Generated from `#/components/schemas/MultichannelSpeechToTextResponseModel/transcription_id`.
            public var transcription_id: Swift.String?
            /// Creates a new `MultichannelSpeechToTextResponseModel`.
            ///
            /// - Parameters:
            ///   - transcripts: List of transcripts, one for each audio channel. Each transcript contains the text and word-level details for its respective channel.
            ///   - transcription_id: The transcription ID of the response.
            public init(
                transcripts: [Components.Schemas.SpeechToTextChunkResponseModel],
                transcription_id: Swift.String? = nil
            ) {
                self.transcripts = transcripts
                self.transcription_id = transcription_id
            }
            public enum CodingKeys: String, CodingKey {
                case transcripts
                case transcription_id
            }
        }
        /// - Remark: Generated from `#/components/schemas/PdfExportOptions`.
        public struct PdfExportOptions: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/PdfExportOptions/include_speakers`.
            public var include_speakers: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/PdfExportOptions/include_timestamps`.
            public var include_timestamps: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/PdfExportOptions/format`.
            @frozen public enum formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case pdf = "pdf"
            }
            /// - Remark: Generated from `#/components/schemas/PdfExportOptions/format`.
            public var format: Components.Schemas.PdfExportOptions.formatPayload
            /// - Remark: Generated from `#/components/schemas/PdfExportOptions/segment_on_silence_longer_than_s`.
            public var segment_on_silence_longer_than_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/PdfExportOptions/max_segment_duration_s`.
            public var max_segment_duration_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/PdfExportOptions/max_segment_chars`.
            public var max_segment_chars: Swift.Int?
            /// Creates a new `PdfExportOptions`.
            ///
            /// - Parameters:
            ///   - include_speakers:
            ///   - include_timestamps:
            ///   - format:
            ///   - segment_on_silence_longer_than_s:
            ///   - max_segment_duration_s:
            ///   - max_segment_chars:
            public init(
                include_speakers: Swift.Bool? = nil,
                include_timestamps: Swift.Bool? = nil,
                format: Components.Schemas.PdfExportOptions.formatPayload,
                segment_on_silence_longer_than_s: Swift.Double? = nil,
                max_segment_duration_s: Swift.Double? = nil,
                max_segment_chars: Swift.Int? = nil
            ) {
                self.include_speakers = include_speakers
                self.include_timestamps = include_timestamps
                self.format = format
                self.segment_on_silence_longer_than_s = segment_on_silence_longer_than_s
                self.max_segment_duration_s = max_segment_duration_s
                self.max_segment_chars = max_segment_chars
            }
            public enum CodingKeys: String, CodingKey {
                case include_speakers
                case include_timestamps
                case format
                case segment_on_silence_longer_than_s
                case max_segment_duration_s
                case max_segment_chars
            }
        }
        /// - Remark: Generated from `#/components/schemas/PronunciationDictionaryVersionLocatorRequestModel`.
        public struct PronunciationDictionaryVersionLocatorRequestModel: Codable, Hashable, Sendable {
            /// The ID of the pronunciation dictionary.
            ///
            /// - Remark: Generated from `#/components/schemas/PronunciationDictionaryVersionLocatorRequestModel/pronunciation_dictionary_id`.
            public var pronunciation_dictionary_id: Swift.String
            /// The ID of the version of the pronunciation dictionary. If not provided, the latest version will be used.
            ///
            /// - Remark: Generated from `#/components/schemas/PronunciationDictionaryVersionLocatorRequestModel/version_id`.
            public var version_id: Swift.String?
            /// Creates a new `PronunciationDictionaryVersionLocatorRequestModel`.
            ///
            /// - Parameters:
            ///   - pronunciation_dictionary_id: The ID of the pronunciation dictionary.
            ///   - version_id: The ID of the version of the pronunciation dictionary. If not provided, the latest version will be used.
            public init(
                pronunciation_dictionary_id: Swift.String,
                version_id: Swift.String? = nil
            ) {
                self.pronunciation_dictionary_id = pronunciation_dictionary_id
                self.version_id = version_id
            }
            public enum CodingKeys: String, CodingKey {
                case pronunciation_dictionary_id
                case version_id
            }
        }
        /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions`.
        public struct SegmentedJsonExportOptions: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions/include_speakers`.
            public var include_speakers: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions/include_timestamps`.
            public var include_timestamps: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions/format`.
            @frozen public enum formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case segmented_json = "segmented_json"
            }
            /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions/format`.
            public var format: Components.Schemas.SegmentedJsonExportOptions.formatPayload
            /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions/segment_on_silence_longer_than_s`.
            public var segment_on_silence_longer_than_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions/max_segment_duration_s`.
            public var max_segment_duration_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/SegmentedJsonExportOptions/max_segment_chars`.
            public var max_segment_chars: Swift.Int?
            /// Creates a new `SegmentedJsonExportOptions`.
            ///
            /// - Parameters:
            ///   - include_speakers:
            ///   - include_timestamps:
            ///   - format:
            ///   - segment_on_silence_longer_than_s:
            ///   - max_segment_duration_s:
            ///   - max_segment_chars:
            public init(
                include_speakers: Swift.Bool? = nil,
                include_timestamps: Swift.Bool? = nil,
                format: Components.Schemas.SegmentedJsonExportOptions.formatPayload,
                segment_on_silence_longer_than_s: Swift.Double? = nil,
                max_segment_duration_s: Swift.Double? = nil,
                max_segment_chars: Swift.Int? = nil
            ) {
                self.include_speakers = include_speakers
                self.include_timestamps = include_timestamps
                self.format = format
                self.segment_on_silence_longer_than_s = segment_on_silence_longer_than_s
                self.max_segment_duration_s = max_segment_duration_s
                self.max_segment_chars = max_segment_chars
            }
            public enum CodingKeys: String, CodingKey {
                case include_speakers
                case include_timestamps
                case format
                case segment_on_silence_longer_than_s
                case max_segment_duration_s
                case max_segment_chars
            }
        }
        /// - Remark: Generated from `#/components/schemas/SpeechToTextCharacterResponseModel`.
        public struct SpeechToTextCharacterResponseModel: Codable, Hashable, Sendable {
            /// The character that was transcribed.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextCharacterResponseModel/text`.
            public var text: Swift.String
            /// The start time of the character in seconds.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextCharacterResponseModel/start`.
            public var start: Swift.Double?
            /// The end time of the character in seconds.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextCharacterResponseModel/end`.
            public var end: Swift.Double?
            /// Creates a new `SpeechToTextCharacterResponseModel`.
            ///
            /// - Parameters:
            ///   - text: The character that was transcribed.
            ///   - start: The start time of the character in seconds.
            ///   - end: The end time of the character in seconds.
            public init(
                text: Swift.String,
                start: Swift.Double? = nil,
                end: Swift.Double? = nil
            ) {
                self.text = text
                self.start = start
                self.end = end
            }
            public enum CodingKeys: String, CodingKey {
                case text
                case start
                case end
            }
        }
        /// Chunk-level detail of the transcription with timing information.
        ///
        /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel`.
        public struct SpeechToTextChunkResponseModel: Codable, Hashable, Sendable {
            /// The detected language code (e.g. 'eng' for English).
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/language_code`.
            public var language_code: Swift.String
            /// The confidence score of the language detection (0 to 1).
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/language_probability`.
            public var language_probability: Swift.Double
            /// The raw text of the transcription.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/text`.
            public var text: Swift.String
            /// List of words with their timing information.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/words`.
            public var words: [Components.Schemas.SpeechToTextWordResponseModel]
            /// The channel index this transcript belongs to (for multichannel audio).
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/channel_index`.
            public var channel_index: Swift.Int?
            /// Requested additional formats of the transcript.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/additional_formats`.
            public var additional_formats: [Components.Schemas.AdditionalFormatResponseModel]?
            /// The transcription ID of the response.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/transcription_id`.
            public var transcription_id: Swift.String?
            /// List of detected entities with their text, type, and character positions in the transcript.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextChunkResponseModel/entities`.
            public var entities: [Components.Schemas.DetectedEntity]?
            /// Creates a new `SpeechToTextChunkResponseModel`.
            ///
            /// - Parameters:
            ///   - language_code: The detected language code (e.g. 'eng' for English).
            ///   - language_probability: The confidence score of the language detection (0 to 1).
            ///   - text: The raw text of the transcription.
            ///   - words: List of words with their timing information.
            ///   - channel_index: The channel index this transcript belongs to (for multichannel audio).
            ///   - additional_formats: Requested additional formats of the transcript.
            ///   - transcription_id: The transcription ID of the response.
            ///   - entities: List of detected entities with their text, type, and character positions in the transcript.
            public init(
                language_code: Swift.String,
                language_probability: Swift.Double,
                text: Swift.String,
                words: [Components.Schemas.SpeechToTextWordResponseModel],
                channel_index: Swift.Int? = nil,
                additional_formats: [Components.Schemas.AdditionalFormatResponseModel]? = nil,
                transcription_id: Swift.String? = nil,
                entities: [Components.Schemas.DetectedEntity]? = nil
            ) {
                self.language_code = language_code
                self.language_probability = language_probability
                self.text = text
                self.words = words
                self.channel_index = channel_index
                self.additional_formats = additional_formats
                self.transcription_id = transcription_id
                self.entities = entities
            }
            public enum CodingKeys: String, CodingKey {
                case language_code
                case language_probability
                case text
                case words
                case channel_index
                case additional_formats
                case transcription_id
                case entities
            }
        }
        /// - Remark: Generated from `#/components/schemas/SpeechToTextWebhookResponseModel`.
        public struct SpeechToTextWebhookResponseModel: Codable, Hashable, Sendable {
            /// The message of the webhook response.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWebhookResponseModel/message`.
            public var message: Swift.String
            /// The request ID of the webhook response.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWebhookResponseModel/request_id`.
            public var request_id: Swift.String
            /// The transcription ID of the webhook response.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWebhookResponseModel/transcription_id`.
            public var transcription_id: Swift.String?
            /// Creates a new `SpeechToTextWebhookResponseModel`.
            ///
            /// - Parameters:
            ///   - message: The message of the webhook response.
            ///   - request_id: The request ID of the webhook response.
            ///   - transcription_id: The transcription ID of the webhook response.
            public init(
                message: Swift.String,
                request_id: Swift.String,
                transcription_id: Swift.String? = nil
            ) {
                self.message = message
                self.request_id = request_id
                self.transcription_id = transcription_id
            }
            public enum CodingKeys: String, CodingKey {
                case message
                case request_id
                case transcription_id
            }
        }
        /// Word-level detail of the transcription with timing information.
        ///
        /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel`.
        public struct SpeechToTextWordResponseModel: Codable, Hashable, Sendable {
            /// The word or sound that was transcribed.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/text`.
            public var text: Swift.String
            /// The start time of the word or sound in seconds.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/start`.
            public var start: Swift.Double?
            /// The end time of the word or sound in seconds.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/end`.
            public var end: Swift.Double?
            /// The type of the word or sound. 'audio_event' is used for non-word sounds like laughter or footsteps.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/type`.
            @frozen public enum _typePayload: String, Codable, Hashable, Sendable, CaseIterable {
                case word = "word"
                case spacing = "spacing"
                case audio_event = "audio_event"
            }
            /// The type of the word or sound. 'audio_event' is used for non-word sounds like laughter or footsteps.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/type`.
            public var _type: Components.Schemas.SpeechToTextWordResponseModel._typePayload
            /// Unique identifier for the speaker of this word.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/speaker_id`.
            public var speaker_id: Swift.String?
            /// The log of the probability with which this word was predicted. Logprobs are in range [-infinity, 0], higher logprobs indicate a higher confidence the model has in its predictions.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/logprob`.
            public var logprob: Swift.Double
            /// The characters that make up the word and their timing information.
            ///
            /// - Remark: Generated from `#/components/schemas/SpeechToTextWordResponseModel/characters`.
            public var characters: [Components.Schemas.SpeechToTextCharacterResponseModel]?
            /// Creates a new `SpeechToTextWordResponseModel`.
            ///
            /// - Parameters:
            ///   - text: The word or sound that was transcribed.
            ///   - start: The start time of the word or sound in seconds.
            ///   - end: The end time of the word or sound in seconds.
            ///   - _type: The type of the word or sound. 'audio_event' is used for non-word sounds like laughter or footsteps.
            ///   - speaker_id: Unique identifier for the speaker of this word.
            ///   - logprob: The log of the probability with which this word was predicted. Logprobs are in range [-infinity, 0], higher logprobs indicate a higher confidence the model has in its predictions.
            ///   - characters: The characters that make up the word and their timing information.
            public init(
                text: Swift.String,
                start: Swift.Double? = nil,
                end: Swift.Double? = nil,
                _type: Components.Schemas.SpeechToTextWordResponseModel._typePayload,
                speaker_id: Swift.String? = nil,
                logprob: Swift.Double,
                characters: [Components.Schemas.SpeechToTextCharacterResponseModel]? = nil
            ) {
                self.text = text
                self.start = start
                self.end = end
                self._type = _type
                self.speaker_id = speaker_id
                self.logprob = logprob
                self.characters = characters
            }
            public enum CodingKeys: String, CodingKey {
                case text
                case start
                case end
                case _type = "type"
                case speaker_id
                case logprob
                case characters
            }
        }
        /// - Remark: Generated from `#/components/schemas/SrtExportOptions`.
        public struct SrtExportOptions: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/max_characters_per_line`.
            public var max_characters_per_line: Swift.Int?
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/include_speakers`.
            public var include_speakers: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/include_timestamps`.
            public var include_timestamps: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/format`.
            @frozen public enum formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case srt = "srt"
            }
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/format`.
            public var format: Components.Schemas.SrtExportOptions.formatPayload
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/segment_on_silence_longer_than_s`.
            public var segment_on_silence_longer_than_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/max_segment_duration_s`.
            public var max_segment_duration_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/SrtExportOptions/max_segment_chars`.
            public var max_segment_chars: Swift.Int?
            /// Creates a new `SrtExportOptions`.
            ///
            /// - Parameters:
            ///   - max_characters_per_line:
            ///   - include_speakers:
            ///   - include_timestamps:
            ///   - format:
            ///   - segment_on_silence_longer_than_s:
            ///   - max_segment_duration_s:
            ///   - max_segment_chars:
            public init(
                max_characters_per_line: Swift.Int? = nil,
                include_speakers: Swift.Bool? = nil,
                include_timestamps: Swift.Bool? = nil,
                format: Components.Schemas.SrtExportOptions.formatPayload,
                segment_on_silence_longer_than_s: Swift.Double? = nil,
                max_segment_duration_s: Swift.Double? = nil,
                max_segment_chars: Swift.Int? = nil
            ) {
                self.max_characters_per_line = max_characters_per_line
                self.include_speakers = include_speakers
                self.include_timestamps = include_timestamps
                self.format = format
                self.segment_on_silence_longer_than_s = segment_on_silence_longer_than_s
                self.max_segment_duration_s = max_segment_duration_s
                self.max_segment_chars = max_segment_chars
            }
            public enum CodingKeys: String, CodingKey {
                case max_characters_per_line
                case include_speakers
                case include_timestamps
                case format
                case segment_on_silence_longer_than_s
                case max_segment_duration_s
                case max_segment_chars
            }
        }
        /// - Remark: Generated from `#/components/schemas/StreamingAudioChunkWithTimestampsResponseModel`.
        public struct StreamingAudioChunkWithTimestampsResponseModel: Codable, Hashable, Sendable {
            /// Base64 encoded audio data
            ///
            /// - Remark: Generated from `#/components/schemas/StreamingAudioChunkWithTimestampsResponseModel/audio_base64`.
            public var audio_base64: Swift.String
            /// Timestamp information for each character in the original text
            ///
            /// - Remark: Generated from `#/components/schemas/StreamingAudioChunkWithTimestampsResponseModel/alignment`.
            public var alignment: Components.Schemas.CharacterAlignmentResponseModel?
            /// Timestamp information for each character in the normalized text
            ///
            /// - Remark: Generated from `#/components/schemas/StreamingAudioChunkWithTimestampsResponseModel/normalized_alignment`.
            public var normalized_alignment: Components.Schemas.CharacterAlignmentResponseModel?
            /// Creates a new `StreamingAudioChunkWithTimestampsResponseModel`.
            ///
            /// - Parameters:
            ///   - audio_base64: Base64 encoded audio data
            ///   - alignment: Timestamp information for each character in the original text
            ///   - normalized_alignment: Timestamp information for each character in the normalized text
            public init(
                audio_base64: Swift.String,
                alignment: Components.Schemas.CharacterAlignmentResponseModel? = nil,
                normalized_alignment: Components.Schemas.CharacterAlignmentResponseModel? = nil
            ) {
                self.audio_base64 = audio_base64
                self.alignment = alignment
                self.normalized_alignment = normalized_alignment
            }
            public enum CodingKeys: String, CodingKey {
                case audio_base64
                case alignment
                case normalized_alignment
            }
        }
        /// - Remark: Generated from `#/components/schemas/TxtExportOptions`.
        public struct TxtExportOptions: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/max_characters_per_line`.
            public var max_characters_per_line: Swift.Int?
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/include_speakers`.
            public var include_speakers: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/include_timestamps`.
            public var include_timestamps: Swift.Bool?
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/format`.
            @frozen public enum formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                case txt = "txt"
            }
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/format`.
            public var format: Components.Schemas.TxtExportOptions.formatPayload
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/segment_on_silence_longer_than_s`.
            public var segment_on_silence_longer_than_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/max_segment_duration_s`.
            public var max_segment_duration_s: Swift.Double?
            /// - Remark: Generated from `#/components/schemas/TxtExportOptions/max_segment_chars`.
            public var max_segment_chars: Swift.Int?
            /// Creates a new `TxtExportOptions`.
            ///
            /// - Parameters:
            ///   - max_characters_per_line:
            ///   - include_speakers:
            ///   - include_timestamps:
            ///   - format:
            ///   - segment_on_silence_longer_than_s:
            ///   - max_segment_duration_s:
            ///   - max_segment_chars:
            public init(
                max_characters_per_line: Swift.Int? = nil,
                include_speakers: Swift.Bool? = nil,
                include_timestamps: Swift.Bool? = nil,
                format: Components.Schemas.TxtExportOptions.formatPayload,
                segment_on_silence_longer_than_s: Swift.Double? = nil,
                max_segment_duration_s: Swift.Double? = nil,
                max_segment_chars: Swift.Int? = nil
            ) {
                self.max_characters_per_line = max_characters_per_line
                self.include_speakers = include_speakers
                self.include_timestamps = include_timestamps
                self.format = format
                self.segment_on_silence_longer_than_s = segment_on_silence_longer_than_s
                self.max_segment_duration_s = max_segment_duration_s
                self.max_segment_chars = max_segment_chars
            }
            public enum CodingKeys: String, CodingKey {
                case max_characters_per_line
                case include_speakers
                case include_timestamps
                case format
                case segment_on_silence_longer_than_s
                case max_segment_duration_s
                case max_segment_chars
            }
        }
        /// - Remark: Generated from `#/components/schemas/ValidationError`.
        public struct ValidationError: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/ValidationError/locPayload`.
            public struct locPayloadPayload: Codable, Hashable, Sendable {
                /// - Remark: Generated from `#/components/schemas/ValidationError/locPayload/value1`.
                public var value1: Swift.String?
                /// - Remark: Generated from `#/components/schemas/ValidationError/locPayload/value2`.
                public var value2: Swift.Int?
                /// Creates a new `locPayloadPayload`.
                ///
                /// - Parameters:
                ///   - value1:
                ///   - value2:
                public init(
                    value1: Swift.String? = nil,
                    value2: Swift.Int? = nil
                ) {
                    self.value1 = value1
                    self.value2 = value2
                }
                public init(from decoder: any Decoder) throws {
                    var errors: [any Error] = []
                    do {
                        self.value1 = try decoder.decodeFromSingleValueContainer()
                    } catch {
                        errors.append(error)
                    }
                    do {
                        self.value2 = try decoder.decodeFromSingleValueContainer()
                    } catch {
                        errors.append(error)
                    }
                    try Swift.DecodingError.verifyAtLeastOneSchemaIsNotNil(
                        [
                            self.value1,
                            self.value2
                        ],
                        type: Self.self,
                        codingPath: decoder.codingPath,
                        errors: errors
                    )
                }
                public func encode(to encoder: any Encoder) throws {
                    try encoder.encodeFirstNonNilValueToSingleValueContainer([
                        self.value1,
                        self.value2
                    ])
                }
            }
            /// - Remark: Generated from `#/components/schemas/ValidationError/loc`.
            public typealias locPayload = [Components.Schemas.ValidationError.locPayloadPayload]
            /// - Remark: Generated from `#/components/schemas/ValidationError/loc`.
            public var loc: Components.Schemas.ValidationError.locPayload
            /// - Remark: Generated from `#/components/schemas/ValidationError/msg`.
            public var msg: Swift.String
            /// - Remark: Generated from `#/components/schemas/ValidationError/type`.
            public var _type: Swift.String
            /// Creates a new `ValidationError`.
            ///
            /// - Parameters:
            ///   - loc:
            ///   - msg:
            ///   - _type:
            public init(
                loc: Components.Schemas.ValidationError.locPayload,
                msg: Swift.String,
                _type: Swift.String
            ) {
                self.loc = loc
                self.msg = msg
                self._type = _type
            }
            public enum CodingKeys: String, CodingKey {
                case loc
                case msg
                case _type = "type"
            }
        }
        /// - Remark: Generated from `#/components/schemas/VoiceSettingsResponseModel`.
        public struct VoiceSettingsResponseModel: Codable, Hashable, Sendable {
            /// Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range for the voice. Higher values can result in a monotonous voice with limited emotion.
            ///
            /// - Remark: Generated from `#/components/schemas/VoiceSettingsResponseModel/stability`.
            public var stability: Swift.Double?
            /// This setting boosts the similarity to the original speaker. Using this setting requires a slightly higher computational load, which in turn increases latency.
            ///
            /// - Remark: Generated from `#/components/schemas/VoiceSettingsResponseModel/use_speaker_boost`.
            public var use_speaker_boost: Swift.Bool?
            /// Determines how closely the AI should adhere to the original voice when attempting to replicate it.
            ///
            /// - Remark: Generated from `#/components/schemas/VoiceSettingsResponseModel/similarity_boost`.
            public var similarity_boost: Swift.Double?
            /// Determines the style exaggeration of the voice. This setting attempts to amplify the style of the original speaker. It does consume additional computational resources and might increase latency if set to anything other than 0.
            ///
            /// - Remark: Generated from `#/components/schemas/VoiceSettingsResponseModel/style`.
            public var style: Swift.Double?
            /// Adjusts the speed of the voice. A value of 1.0 is the default speed, while values less than 1.0 slow down the speech, and values greater than 1.0 speed it up.
            ///
            /// - Remark: Generated from `#/components/schemas/VoiceSettingsResponseModel/speed`.
            public var speed: Swift.Double?
            /// Creates a new `VoiceSettingsResponseModel`.
            ///
            /// - Parameters:
            ///   - stability: Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range for the voice. Higher values can result in a monotonous voice with limited emotion.
            ///   - use_speaker_boost: This setting boosts the similarity to the original speaker. Using this setting requires a slightly higher computational load, which in turn increases latency.
            ///   - similarity_boost: Determines how closely the AI should adhere to the original voice when attempting to replicate it.
            ///   - style: Determines the style exaggeration of the voice. This setting attempts to amplify the style of the original speaker. It does consume additional computational resources and might increase latency if set to anything other than 0.
            ///   - speed: Adjusts the speed of the voice. A value of 1.0 is the default speed, while values less than 1.0 slow down the speech, and values greater than 1.0 speed it up.
            public init(
                stability: Swift.Double? = nil,
                use_speaker_boost: Swift.Bool? = nil,
                similarity_boost: Swift.Double? = nil,
                style: Swift.Double? = nil,
                speed: Swift.Double? = nil
            ) {
                self.stability = stability
                self.use_speaker_boost = use_speaker_boost
                self.similarity_boost = similarity_boost
                self.style = style
                self.speed = speed
            }
            public enum CodingKeys: String, CodingKey {
                case stability
                case use_speaker_boost
                case similarity_boost
                case style
                case speed
            }
        }
        /// - Remark: Generated from `#/components/schemas/PaymentRequiredErrorDetail`.
        public struct PaymentRequiredErrorDetail: Codable, Hashable, Sendable {
            /// Error code identifying the specific payment issue.
            ///
            /// - Remark: Generated from `#/components/schemas/PaymentRequiredErrorDetail/code`.
            public var code: Swift.String?
            /// Human-readable description of the payment requirement.
            ///
            /// - Remark: Generated from `#/components/schemas/PaymentRequiredErrorDetail/message`.
            public var message: Swift.String?
            /// Unique identifier for the request.
            ///
            /// - Remark: Generated from `#/components/schemas/PaymentRequiredErrorDetail/request_id`.
            public var request_id: Swift.String?
            /// HTTP status description.
            ///
            /// - Remark: Generated from `#/components/schemas/PaymentRequiredErrorDetail/status`.
            public var status: Swift.String?
            /// Error type.
            ///
            /// - Remark: Generated from `#/components/schemas/PaymentRequiredErrorDetail/type`.
            public var _type: Swift.String?
            /// Creates a new `PaymentRequiredErrorDetail`.
            ///
            /// - Parameters:
            ///   - code: Error code identifying the specific payment issue.
            ///   - message: Human-readable description of the payment requirement.
            ///   - request_id: Unique identifier for the request.
            ///   - status: HTTP status description.
            ///   - _type: Error type.
            public init(
                code: Swift.String? = nil,
                message: Swift.String? = nil,
                request_id: Swift.String? = nil,
                status: Swift.String? = nil,
                _type: Swift.String? = nil
            ) {
                self.code = code
                self.message = message
                self.request_id = request_id
                self.status = status
                self._type = _type
            }
            public enum CodingKeys: String, CodingKey {
                case code
                case message
                case request_id
                case status
                case _type = "type"
            }
        }
        /// - Remark: Generated from `#/components/schemas/PaymentRequiredError`.
        public struct PaymentRequiredError: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/PaymentRequiredError/detail`.
            public var detail: Components.Schemas.PaymentRequiredErrorDetail?
            /// Creates a new `PaymentRequiredError`.
            ///
            /// - Parameters:
            ///   - detail:
            public init(detail: Components.Schemas.PaymentRequiredErrorDetail? = nil) {
                self.detail = detail
            }
            public enum CodingKeys: String, CodingKey {
                case detail
            }
        }
        /// - Remark: Generated from `#/components/schemas/BadRequestErrorDetail`.
        public struct BadRequestErrorDetail: Codable, Hashable, Sendable {
            /// Error code identifying the specific issue.
            ///
            /// - Remark: Generated from `#/components/schemas/BadRequestErrorDetail/status`.
            public var status: Swift.String?
            /// Human-readable description of the error.
            ///
            /// - Remark: Generated from `#/components/schemas/BadRequestErrorDetail/message`.
            public var message: Swift.String?
            /// Creates a new `BadRequestErrorDetail`.
            ///
            /// - Parameters:
            ///   - status: Error code identifying the specific issue.
            ///   - message: Human-readable description of the error.
            public init(
                status: Swift.String? = nil,
                message: Swift.String? = nil
            ) {
                self.status = status
                self.message = message
            }
            public enum CodingKeys: String, CodingKey {
                case status
                case message
            }
        }
        /// - Remark: Generated from `#/components/schemas/BadRequestError`.
        public struct BadRequestError: Codable, Hashable, Sendable {
            /// - Remark: Generated from `#/components/schemas/BadRequestError/detail`.
            public var detail: Components.Schemas.BadRequestErrorDetail?
            /// Creates a new `BadRequestError`.
            ///
            /// - Parameters:
            ///   - detail:
            public init(detail: Components.Schemas.BadRequestErrorDetail? = nil) {
                self.detail = detail
            }
            public enum CodingKeys: String, CodingKey {
                case detail
            }
        }
    }
    /// Types generated from the `#/components/parameters` section of the OpenAPI document.
    public enum Parameters {}
    /// Types generated from the `#/components/requestBodies` section of the OpenAPI document.
    public enum RequestBodies {}
    /// Types generated from the `#/components/responses` section of the OpenAPI document.
    public enum Responses {
        public struct PaymentRequired: Sendable, Hashable {
            /// - Remark: Generated from `#/components/responses/PaymentRequired/content`.
            @frozen public enum Body: Sendable, Hashable {
                /// - Remark: Generated from `#/components/responses/PaymentRequired/content/application\/json`.
                case json(Components.Schemas.PaymentRequiredError)
                /// The associated value of the enum case if `self` is `.json`.
                ///
                /// - Throws: An error if `self` is not `.json`.
                /// - SeeAlso: `.json`.
                public var json: Components.Schemas.PaymentRequiredError {
                    get throws {
                        switch self {
                        case let .json(body):
                            return body
                        }
                    }
                }
            }
            /// Received HTTP response body
            public var body: Components.Responses.PaymentRequired.Body
            /// Creates a new `PaymentRequired`.
            ///
            /// - Parameters:
            ///   - body: Received HTTP response body
            public init(body: Components.Responses.PaymentRequired.Body) {
                self.body = body
            }
        }
        public struct BadRequest: Sendable, Hashable {
            /// - Remark: Generated from `#/components/responses/BadRequest/content`.
            @frozen public enum Body: Sendable, Hashable {
                /// - Remark: Generated from `#/components/responses/BadRequest/content/application\/json`.
                case json(Components.Schemas.BadRequestError)
                /// The associated value of the enum case if `self` is `.json`.
                ///
                /// - Throws: An error if `self` is not `.json`.
                /// - SeeAlso: `.json`.
                public var json: Components.Schemas.BadRequestError {
                    get throws {
                        switch self {
                        case let .json(body):
                            return body
                        }
                    }
                }
            }
            /// Received HTTP response body
            public var body: Components.Responses.BadRequest.Body
            /// Creates a new `BadRequest`.
            ///
            /// - Parameters:
            ///   - body: Received HTTP response body
            public init(body: Components.Responses.BadRequest.Body) {
                self.body = body
            }
        }
    }
    /// Types generated from the `#/components/headers` section of the OpenAPI document.
    public enum Headers {}
}

/// API operations, with input and output types, generated from `#/paths` in the OpenAPI document.
public enum Operations {
    /// Text To Speech
    ///
    /// Converts text into speech using a voice of your choice and returns audio.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/post(text_to_speech_full)`.
    public enum text_to_speech_full {
        public static let id: Swift.String = "text_to_speech_full"
        public struct Input: Sendable, Hashable {
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/path`.
            public struct Path: Sendable, Hashable {
                /// Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/path/voice_id`.
                public var voice_id: Swift.String
                /// Creates a new `Path`.
                ///
                /// - Parameters:
                ///   - voice_id: Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                public init(voice_id: Swift.String) {
                    self.voice_id = voice_id
                }
            }
            public var path: Operations.text_to_speech_full.Input.Path
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/query`.
            public struct Query: Sendable, Hashable {
                /// When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/query/enable_logging`.
                public var enable_logging: Swift.Bool?
                /// You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                /// 0 - default mode (no latency optimizations)
                /// 1 - normal latency optimizations (about 50% of possible latency improvement of option 3)
                /// 2 - strong latency optimizations (about 75% of possible latency improvement of option 3)
                /// 3 - max latency optimizations
                /// 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
                ///
                /// Defaults to None.
                ///
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/query/optimize_streaming_latency`.
                @available(*, deprecated)
                public var optimize_streaming_latency: Swift.Int?
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM and WAV formats with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/query/output_format`.
                @frozen public enum output_formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                    case alaw_8000 = "alaw_8000"
                    case mp3_22050_32 = "mp3_22050_32"
                    case mp3_24000_48 = "mp3_24000_48"
                    case mp3_44100_128 = "mp3_44100_128"
                    case mp3_44100_192 = "mp3_44100_192"
                    case mp3_44100_32 = "mp3_44100_32"
                    case mp3_44100_64 = "mp3_44100_64"
                    case mp3_44100_96 = "mp3_44100_96"
                    case opus_48000_128 = "opus_48000_128"
                    case opus_48000_192 = "opus_48000_192"
                    case opus_48000_32 = "opus_48000_32"
                    case opus_48000_64 = "opus_48000_64"
                    case opus_48000_96 = "opus_48000_96"
                    case pcm_16000 = "pcm_16000"
                    case pcm_22050 = "pcm_22050"
                    case pcm_24000 = "pcm_24000"
                    case pcm_32000 = "pcm_32000"
                    case pcm_44100 = "pcm_44100"
                    case pcm_48000 = "pcm_48000"
                    case pcm_8000 = "pcm_8000"
                    case ulaw_8000 = "ulaw_8000"
                    case wav_16000 = "wav_16000"
                    case wav_22050 = "wav_22050"
                    case wav_24000 = "wav_24000"
                    case wav_32000 = "wav_32000"
                    case wav_44100 = "wav_44100"
                    case wav_48000 = "wav_48000"
                    case wav_8000 = "wav_8000"
                }
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM and WAV formats with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/query/output_format`.
                public var output_format: Operations.text_to_speech_full.Input.Query.output_formatPayload?
                /// Creates a new `Query`.
                ///
                /// - Parameters:
                ///   - enable_logging: When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///   - optimize_streaming_latency: You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                ///   - output_format: Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM and WAV formats with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                public init(
                    enable_logging: Swift.Bool? = nil,
                    optimize_streaming_latency: Swift.Int? = nil,
                    output_format: Operations.text_to_speech_full.Input.Query.output_formatPayload? = nil
                ) {
                    self.enable_logging = enable_logging
                    self.optimize_streaming_latency = optimize_streaming_latency
                    self.output_format = output_format
                }
            }
            public var query: Operations.text_to_speech_full.Input.Query
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/header`.
            public struct Headers: Sendable, Hashable {
                /// Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/header/xi-api-key`.
                public var xi_hyphen_api_hyphen_key: Swift.String?
                public var accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_full.AcceptableContentType>]
                /// Creates a new `Headers`.
                ///
                /// - Parameters:
                ///   - xi_hyphen_api_hyphen_key: Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///   - accept:
                public init(
                    xi_hyphen_api_hyphen_key: Swift.String? = nil,
                    accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_full.AcceptableContentType>] = .defaultValues()
                ) {
                    self.xi_hyphen_api_hyphen_key = xi_hyphen_api_hyphen_key
                    self.accept = accept
                }
            }
            public var headers: Operations.text_to_speech_full.Input.Headers
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/requestBody`.
            @frozen public enum Body: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/requestBody/content/application\/json`.
                case json(Components.Schemas.Body_text_to_speech_full)
            }
            public var body: Operations.text_to_speech_full.Input.Body
            /// Creates a new `Input`.
            ///
            /// - Parameters:
            ///   - path:
            ///   - query:
            ///   - headers:
            ///   - body:
            public init(
                path: Operations.text_to_speech_full.Input.Path,
                query: Operations.text_to_speech_full.Input.Query = .init(),
                headers: Operations.text_to_speech_full.Input.Headers = .init(),
                body: Operations.text_to_speech_full.Input.Body
            ) {
                self.path = path
                self.query = query
                self.headers = headers
                self.body = body
            }
        }
        @frozen public enum Output: Sendable, Hashable {
            public struct Ok: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/responses/200/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/responses/200/content/audio\/mpeg`.
                    case audio_mpeg(OpenAPIRuntime.HTTPBody)
                    /// The associated value of the enum case if `self` is `.audio_mpeg`.
                    ///
                    /// - Throws: An error if `self` is not `.audio_mpeg`.
                    /// - SeeAlso: `.audio_mpeg`.
                    public var audio_mpeg: OpenAPIRuntime.HTTPBody {
                        get throws {
                            switch self {
                            case let .audio_mpeg(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_full.Output.Ok.Body
                /// Creates a new `Ok`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_full.Output.Ok.Body) {
                    self.body = body
                }
            }
            /// The generated audio file
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/post(text_to_speech_full)/responses/200`.
            ///
            /// HTTP response code: `200 ok`.
            case ok(Operations.text_to_speech_full.Output.Ok)
            /// The associated value of the enum case if `self` is `.ok`.
            ///
            /// - Throws: An error if `self` is not `.ok`.
            /// - SeeAlso: `.ok`.
            public var ok: Operations.text_to_speech_full.Output.Ok {
                get throws {
                    switch self {
                    case let .ok(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "ok",
                            response: self
                        )
                    }
                }
            }
            /// Bad Request - The request was invalid or malformed.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/post(text_to_speech_full)/responses/400`.
            ///
            /// HTTP response code: `400 badRequest`.
            case badRequest(Components.Responses.BadRequest)
            /// The associated value of the enum case if `self` is `.badRequest`.
            ///
            /// - Throws: An error if `self` is not `.badRequest`.
            /// - SeeAlso: `.badRequest`.
            public var badRequest: Components.Responses.BadRequest {
                get throws {
                    switch self {
                    case let .badRequest(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "badRequest",
                            response: self
                        )
                    }
                }
            }
            /// Payment Required - A paid plan is required to access this resource.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/post(text_to_speech_full)/responses/402`.
            ///
            /// HTTP response code: `402 code402`.
            case code402(Components.Responses.PaymentRequired)
            /// The associated value of the enum case if `self` is `.code402`.
            ///
            /// - Throws: An error if `self` is not `.code402`.
            /// - SeeAlso: `.code402`.
            public var code402: Components.Responses.PaymentRequired {
                get throws {
                    switch self {
                    case let .code402(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "code402",
                            response: self
                        )
                    }
                }
            }
            public struct UnprocessableContent: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/responses/422/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/POST/responses/422/content/application\/json`.
                    case json(Components.Schemas.HTTPValidationError)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.HTTPValidationError {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_full.Output.UnprocessableContent.Body
                /// Creates a new `UnprocessableContent`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_full.Output.UnprocessableContent.Body) {
                    self.body = body
                }
            }
            /// Validation Error
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/post(text_to_speech_full)/responses/422`.
            ///
            /// HTTP response code: `422 unprocessableContent`.
            case unprocessableContent(Operations.text_to_speech_full.Output.UnprocessableContent)
            /// The associated value of the enum case if `self` is `.unprocessableContent`.
            ///
            /// - Throws: An error if `self` is not `.unprocessableContent`.
            /// - SeeAlso: `.unprocessableContent`.
            public var unprocessableContent: Operations.text_to_speech_full.Output.UnprocessableContent {
                get throws {
                    switch self {
                    case let .unprocessableContent(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unprocessableContent",
                            response: self
                        )
                    }
                }
            }
            /// Undocumented response.
            ///
            /// A response with a code that is not documented in the OpenAPI document.
            case undocumented(statusCode: Swift.Int, OpenAPIRuntime.UndocumentedPayload)
        }
        @frozen public enum AcceptableContentType: AcceptableProtocol {
            case audio_mpeg
            case json
            case other(Swift.String)
            public init?(rawValue: Swift.String) {
                switch rawValue.lowercased() {
                case "audio/mpeg":
                    self = .audio_mpeg
                case "application/json":
                    self = .json
                default:
                    self = .other(rawValue)
                }
            }
            public var rawValue: Swift.String {
                switch self {
                case let .other(string):
                    return string
                case .audio_mpeg:
                    return "audio/mpeg"
                case .json:
                    return "application/json"
                }
            }
            public static var allCases: [Self] {
                [
                    .audio_mpeg,
                    .json
                ]
            }
        }
    }
    /// Text To Speech With Timestamps
    ///
    /// Generate speech from text with precise character-level timing information for audio-text synchronization.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/with-timestamps`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/with-timestamps/post(text_to_speech_full_with_timestamps)`.
    public enum text_to_speech_full_with_timestamps {
        public static let id: Swift.String = "text_to_speech_full_with_timestamps"
        public struct Input: Sendable, Hashable {
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/path`.
            public struct Path: Sendable, Hashable {
                /// Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/path/voice_id`.
                public var voice_id: Swift.String
                /// Creates a new `Path`.
                ///
                /// - Parameters:
                ///   - voice_id: Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                public init(voice_id: Swift.String) {
                    self.voice_id = voice_id
                }
            }
            public var path: Operations.text_to_speech_full_with_timestamps.Input.Path
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/query`.
            public struct Query: Sendable, Hashable {
                /// When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/query/enable_logging`.
                public var enable_logging: Swift.Bool?
                /// You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                /// 0 - default mode (no latency optimizations)
                /// 1 - normal latency optimizations (about 50% of possible latency improvement of option 3)
                /// 2 - strong latency optimizations (about 75% of possible latency improvement of option 3)
                /// 3 - max latency optimizations
                /// 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
                ///
                /// Defaults to None.
                ///
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/query/optimize_streaming_latency`.
                @available(*, deprecated)
                public var optimize_streaming_latency: Swift.Int?
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM and WAV formats with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/query/output_format`.
                @frozen public enum output_formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                    case alaw_8000 = "alaw_8000"
                    case mp3_22050_32 = "mp3_22050_32"
                    case mp3_24000_48 = "mp3_24000_48"
                    case mp3_44100_128 = "mp3_44100_128"
                    case mp3_44100_192 = "mp3_44100_192"
                    case mp3_44100_32 = "mp3_44100_32"
                    case mp3_44100_64 = "mp3_44100_64"
                    case mp3_44100_96 = "mp3_44100_96"
                    case opus_48000_128 = "opus_48000_128"
                    case opus_48000_192 = "opus_48000_192"
                    case opus_48000_32 = "opus_48000_32"
                    case opus_48000_64 = "opus_48000_64"
                    case opus_48000_96 = "opus_48000_96"
                    case pcm_16000 = "pcm_16000"
                    case pcm_22050 = "pcm_22050"
                    case pcm_24000 = "pcm_24000"
                    case pcm_32000 = "pcm_32000"
                    case pcm_44100 = "pcm_44100"
                    case pcm_48000 = "pcm_48000"
                    case pcm_8000 = "pcm_8000"
                    case ulaw_8000 = "ulaw_8000"
                    case wav_16000 = "wav_16000"
                    case wav_22050 = "wav_22050"
                    case wav_24000 = "wav_24000"
                    case wav_32000 = "wav_32000"
                    case wav_44100 = "wav_44100"
                    case wav_48000 = "wav_48000"
                    case wav_8000 = "wav_8000"
                }
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM and WAV formats with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/query/output_format`.
                public var output_format: Operations.text_to_speech_full_with_timestamps.Input.Query.output_formatPayload?
                /// Creates a new `Query`.
                ///
                /// - Parameters:
                ///   - enable_logging: When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///   - optimize_streaming_latency: You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                ///   - output_format: Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM and WAV formats with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                public init(
                    enable_logging: Swift.Bool? = nil,
                    optimize_streaming_latency: Swift.Int? = nil,
                    output_format: Operations.text_to_speech_full_with_timestamps.Input.Query.output_formatPayload? = nil
                ) {
                    self.enable_logging = enable_logging
                    self.optimize_streaming_latency = optimize_streaming_latency
                    self.output_format = output_format
                }
            }
            public var query: Operations.text_to_speech_full_with_timestamps.Input.Query
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/header`.
            public struct Headers: Sendable, Hashable {
                /// Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/header/xi-api-key`.
                public var xi_hyphen_api_hyphen_key: Swift.String?
                public var accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_full_with_timestamps.AcceptableContentType>]
                /// Creates a new `Headers`.
                ///
                /// - Parameters:
                ///   - xi_hyphen_api_hyphen_key: Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///   - accept:
                public init(
                    xi_hyphen_api_hyphen_key: Swift.String? = nil,
                    accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_full_with_timestamps.AcceptableContentType>] = .defaultValues()
                ) {
                    self.xi_hyphen_api_hyphen_key = xi_hyphen_api_hyphen_key
                    self.accept = accept
                }
            }
            public var headers: Operations.text_to_speech_full_with_timestamps.Input.Headers
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/requestBody`.
            @frozen public enum Body: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/requestBody/content/application\/json`.
                case json(Components.Schemas.Body_text_to_speech_full_with_timestamps)
            }
            public var body: Operations.text_to_speech_full_with_timestamps.Input.Body
            /// Creates a new `Input`.
            ///
            /// - Parameters:
            ///   - path:
            ///   - query:
            ///   - headers:
            ///   - body:
            public init(
                path: Operations.text_to_speech_full_with_timestamps.Input.Path,
                query: Operations.text_to_speech_full_with_timestamps.Input.Query = .init(),
                headers: Operations.text_to_speech_full_with_timestamps.Input.Headers = .init(),
                body: Operations.text_to_speech_full_with_timestamps.Input.Body
            ) {
                self.path = path
                self.query = query
                self.headers = headers
                self.body = body
            }
        }
        @frozen public enum Output: Sendable, Hashable {
            public struct Ok: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/responses/200/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/responses/200/content/application\/json`.
                    case json(Components.Schemas.AudioWithTimestampsResponseModel)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.AudioWithTimestampsResponseModel {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_full_with_timestamps.Output.Ok.Body
                /// Creates a new `Ok`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_full_with_timestamps.Output.Ok.Body) {
                    self.body = body
                }
            }
            /// Successful Response
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/with-timestamps/post(text_to_speech_full_with_timestamps)/responses/200`.
            ///
            /// HTTP response code: `200 ok`.
            case ok(Operations.text_to_speech_full_with_timestamps.Output.Ok)
            /// The associated value of the enum case if `self` is `.ok`.
            ///
            /// - Throws: An error if `self` is not `.ok`.
            /// - SeeAlso: `.ok`.
            public var ok: Operations.text_to_speech_full_with_timestamps.Output.Ok {
                get throws {
                    switch self {
                    case let .ok(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "ok",
                            response: self
                        )
                    }
                }
            }
            /// Bad Request - The request was invalid or malformed.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/with-timestamps/post(text_to_speech_full_with_timestamps)/responses/400`.
            ///
            /// HTTP response code: `400 badRequest`.
            case badRequest(Components.Responses.BadRequest)
            /// The associated value of the enum case if `self` is `.badRequest`.
            ///
            /// - Throws: An error if `self` is not `.badRequest`.
            /// - SeeAlso: `.badRequest`.
            public var badRequest: Components.Responses.BadRequest {
                get throws {
                    switch self {
                    case let .badRequest(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "badRequest",
                            response: self
                        )
                    }
                }
            }
            /// Payment Required - A paid plan is required to access this resource.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/with-timestamps/post(text_to_speech_full_with_timestamps)/responses/402`.
            ///
            /// HTTP response code: `402 code402`.
            case code402(Components.Responses.PaymentRequired)
            /// The associated value of the enum case if `self` is `.code402`.
            ///
            /// - Throws: An error if `self` is not `.code402`.
            /// - SeeAlso: `.code402`.
            public var code402: Components.Responses.PaymentRequired {
                get throws {
                    switch self {
                    case let .code402(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "code402",
                            response: self
                        )
                    }
                }
            }
            public struct UnprocessableContent: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/responses/422/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/with-timestamps/POST/responses/422/content/application\/json`.
                    case json(Components.Schemas.HTTPValidationError)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.HTTPValidationError {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_full_with_timestamps.Output.UnprocessableContent.Body
                /// Creates a new `UnprocessableContent`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_full_with_timestamps.Output.UnprocessableContent.Body) {
                    self.body = body
                }
            }
            /// Validation Error
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/with-timestamps/post(text_to_speech_full_with_timestamps)/responses/422`.
            ///
            /// HTTP response code: `422 unprocessableContent`.
            case unprocessableContent(Operations.text_to_speech_full_with_timestamps.Output.UnprocessableContent)
            /// The associated value of the enum case if `self` is `.unprocessableContent`.
            ///
            /// - Throws: An error if `self` is not `.unprocessableContent`.
            /// - SeeAlso: `.unprocessableContent`.
            public var unprocessableContent: Operations.text_to_speech_full_with_timestamps.Output.UnprocessableContent {
                get throws {
                    switch self {
                    case let .unprocessableContent(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unprocessableContent",
                            response: self
                        )
                    }
                }
            }
            /// Undocumented response.
            ///
            /// A response with a code that is not documented in the OpenAPI document.
            case undocumented(statusCode: Swift.Int, OpenAPIRuntime.UndocumentedPayload)
        }
        @frozen public enum AcceptableContentType: AcceptableProtocol {
            case json
            case other(Swift.String)
            public init?(rawValue: Swift.String) {
                switch rawValue.lowercased() {
                case "application/json":
                    self = .json
                default:
                    self = .other(rawValue)
                }
            }
            public var rawValue: Swift.String {
                switch self {
                case let .other(string):
                    return string
                case .json:
                    return "application/json"
                }
            }
            public static var allCases: [Self] {
                [
                    .json
                ]
            }
        }
    }
    /// Text To Speech Streaming
    ///
    /// Converts text into speech using a voice of your choice and returns audio as an audio stream.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/stream`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/post(text_to_speech_stream)`.
    public enum text_to_speech_stream {
        public static let id: Swift.String = "text_to_speech_stream"
        public struct Input: Sendable, Hashable {
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/path`.
            public struct Path: Sendable, Hashable {
                /// Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/path/voice_id`.
                public var voice_id: Swift.String
                /// Creates a new `Path`.
                ///
                /// - Parameters:
                ///   - voice_id: Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                public init(voice_id: Swift.String) {
                    self.voice_id = voice_id
                }
            }
            public var path: Operations.text_to_speech_stream.Input.Path
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/query`.
            public struct Query: Sendable, Hashable {
                /// When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/query/enable_logging`.
                public var enable_logging: Swift.Bool?
                /// You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                /// 0 - default mode (no latency optimizations)
                /// 1 - normal latency optimizations (about 50% of possible latency improvement of option 3)
                /// 2 - strong latency optimizations (about 75% of possible latency improvement of option 3)
                /// 3 - max latency optimizations
                /// 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
                ///
                /// Defaults to None.
                ///
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/query/optimize_streaming_latency`.
                @available(*, deprecated)
                public var optimize_streaming_latency: Swift.Int?
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/query/output_format`.
                @frozen public enum output_formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                    case mp3_22050_32 = "mp3_22050_32"
                    case mp3_24000_48 = "mp3_24000_48"
                    case mp3_44100_32 = "mp3_44100_32"
                    case mp3_44100_64 = "mp3_44100_64"
                    case mp3_44100_96 = "mp3_44100_96"
                    case mp3_44100_128 = "mp3_44100_128"
                    case mp3_44100_192 = "mp3_44100_192"
                    case pcm_8000 = "pcm_8000"
                    case pcm_16000 = "pcm_16000"
                    case pcm_22050 = "pcm_22050"
                    case pcm_24000 = "pcm_24000"
                    case pcm_32000 = "pcm_32000"
                    case pcm_44100 = "pcm_44100"
                    case pcm_48000 = "pcm_48000"
                    case ulaw_8000 = "ulaw_8000"
                    case alaw_8000 = "alaw_8000"
                    case opus_48000_32 = "opus_48000_32"
                    case opus_48000_64 = "opus_48000_64"
                    case opus_48000_96 = "opus_48000_96"
                    case opus_48000_128 = "opus_48000_128"
                    case opus_48000_192 = "opus_48000_192"
                }
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/query/output_format`.
                public var output_format: Operations.text_to_speech_stream.Input.Query.output_formatPayload?
                /// Creates a new `Query`.
                ///
                /// - Parameters:
                ///   - enable_logging: When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///   - optimize_streaming_latency: You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                ///   - output_format: Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                public init(
                    enable_logging: Swift.Bool? = nil,
                    optimize_streaming_latency: Swift.Int? = nil,
                    output_format: Operations.text_to_speech_stream.Input.Query.output_formatPayload? = nil
                ) {
                    self.enable_logging = enable_logging
                    self.optimize_streaming_latency = optimize_streaming_latency
                    self.output_format = output_format
                }
            }
            public var query: Operations.text_to_speech_stream.Input.Query
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/header`.
            public struct Headers: Sendable, Hashable {
                /// Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/header/xi-api-key`.
                public var xi_hyphen_api_hyphen_key: Swift.String?
                public var accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_stream.AcceptableContentType>]
                /// Creates a new `Headers`.
                ///
                /// - Parameters:
                ///   - xi_hyphen_api_hyphen_key: Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///   - accept:
                public init(
                    xi_hyphen_api_hyphen_key: Swift.String? = nil,
                    accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_stream.AcceptableContentType>] = .defaultValues()
                ) {
                    self.xi_hyphen_api_hyphen_key = xi_hyphen_api_hyphen_key
                    self.accept = accept
                }
            }
            public var headers: Operations.text_to_speech_stream.Input.Headers
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/requestBody`.
            @frozen public enum Body: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/requestBody/content/application\/json`.
                case json(Components.Schemas.Body_text_to_speech_stream)
            }
            public var body: Operations.text_to_speech_stream.Input.Body
            /// Creates a new `Input`.
            ///
            /// - Parameters:
            ///   - path:
            ///   - query:
            ///   - headers:
            ///   - body:
            public init(
                path: Operations.text_to_speech_stream.Input.Path,
                query: Operations.text_to_speech_stream.Input.Query = .init(),
                headers: Operations.text_to_speech_stream.Input.Headers = .init(),
                body: Operations.text_to_speech_stream.Input.Body
            ) {
                self.path = path
                self.query = query
                self.headers = headers
                self.body = body
            }
        }
        @frozen public enum Output: Sendable, Hashable {
            public struct Ok: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/responses/200/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/responses/200/content/audio\/mpeg`.
                    case audio_mpeg(OpenAPIRuntime.HTTPBody)
                    /// The associated value of the enum case if `self` is `.audio_mpeg`.
                    ///
                    /// - Throws: An error if `self` is not `.audio_mpeg`.
                    /// - SeeAlso: `.audio_mpeg`.
                    public var audio_mpeg: OpenAPIRuntime.HTTPBody {
                        get throws {
                            switch self {
                            case let .audio_mpeg(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_stream.Output.Ok.Body
                /// Creates a new `Ok`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_stream.Output.Ok.Body) {
                    self.body = body
                }
            }
            /// Streaming audio data
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/post(text_to_speech_stream)/responses/200`.
            ///
            /// HTTP response code: `200 ok`.
            case ok(Operations.text_to_speech_stream.Output.Ok)
            /// The associated value of the enum case if `self` is `.ok`.
            ///
            /// - Throws: An error if `self` is not `.ok`.
            /// - SeeAlso: `.ok`.
            public var ok: Operations.text_to_speech_stream.Output.Ok {
                get throws {
                    switch self {
                    case let .ok(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "ok",
                            response: self
                        )
                    }
                }
            }
            /// Bad Request - The request was invalid or malformed.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/post(text_to_speech_stream)/responses/400`.
            ///
            /// HTTP response code: `400 badRequest`.
            case badRequest(Components.Responses.BadRequest)
            /// The associated value of the enum case if `self` is `.badRequest`.
            ///
            /// - Throws: An error if `self` is not `.badRequest`.
            /// - SeeAlso: `.badRequest`.
            public var badRequest: Components.Responses.BadRequest {
                get throws {
                    switch self {
                    case let .badRequest(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "badRequest",
                            response: self
                        )
                    }
                }
            }
            /// Payment Required - A paid plan is required to access this resource.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/post(text_to_speech_stream)/responses/402`.
            ///
            /// HTTP response code: `402 code402`.
            case code402(Components.Responses.PaymentRequired)
            /// The associated value of the enum case if `self` is `.code402`.
            ///
            /// - Throws: An error if `self` is not `.code402`.
            /// - SeeAlso: `.code402`.
            public var code402: Components.Responses.PaymentRequired {
                get throws {
                    switch self {
                    case let .code402(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "code402",
                            response: self
                        )
                    }
                }
            }
            public struct UnprocessableContent: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/responses/422/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/POST/responses/422/content/application\/json`.
                    case json(Components.Schemas.HTTPValidationError)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.HTTPValidationError {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_stream.Output.UnprocessableContent.Body
                /// Creates a new `UnprocessableContent`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_stream.Output.UnprocessableContent.Body) {
                    self.body = body
                }
            }
            /// Validation Error
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/post(text_to_speech_stream)/responses/422`.
            ///
            /// HTTP response code: `422 unprocessableContent`.
            case unprocessableContent(Operations.text_to_speech_stream.Output.UnprocessableContent)
            /// The associated value of the enum case if `self` is `.unprocessableContent`.
            ///
            /// - Throws: An error if `self` is not `.unprocessableContent`.
            /// - SeeAlso: `.unprocessableContent`.
            public var unprocessableContent: Operations.text_to_speech_stream.Output.UnprocessableContent {
                get throws {
                    switch self {
                    case let .unprocessableContent(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unprocessableContent",
                            response: self
                        )
                    }
                }
            }
            /// Undocumented response.
            ///
            /// A response with a code that is not documented in the OpenAPI document.
            case undocumented(statusCode: Swift.Int, OpenAPIRuntime.UndocumentedPayload)
        }
        @frozen public enum AcceptableContentType: AcceptableProtocol {
            case audio_mpeg
            case json
            case other(Swift.String)
            public init?(rawValue: Swift.String) {
                switch rawValue.lowercased() {
                case "audio/mpeg":
                    self = .audio_mpeg
                case "application/json":
                    self = .json
                default:
                    self = .other(rawValue)
                }
            }
            public var rawValue: Swift.String {
                switch self {
                case let .other(string):
                    return string
                case .audio_mpeg:
                    return "audio/mpeg"
                case .json:
                    return "application/json"
                }
            }
            public static var allCases: [Self] {
                [
                    .audio_mpeg,
                    .json
                ]
            }
        }
    }
    /// Text To Speech Streaming With Timestamps
    ///
    /// Converts text into speech using a voice of your choice and returns a stream of JSONs containing audio as a base64 encoded string together with information on when which character was spoken.
    ///
    /// - Remark: HTTP `POST /v1/text-to-speech/{voice_id}/stream/with-timestamps`.
    /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/with-timestamps/post(text_to_speech_stream_with_timestamps)`.
    public enum text_to_speech_stream_with_timestamps {
        public static let id: Swift.String = "text_to_speech_stream_with_timestamps"
        public struct Input: Sendable, Hashable {
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/path`.
            public struct Path: Sendable, Hashable {
                /// Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/path/voice_id`.
                public var voice_id: Swift.String
                /// Creates a new `Path`.
                ///
                /// - Parameters:
                ///   - voice_id: Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
                public init(voice_id: Swift.String) {
                    self.voice_id = voice_id
                }
            }
            public var path: Operations.text_to_speech_stream_with_timestamps.Input.Path
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/query`.
            public struct Query: Sendable, Hashable {
                /// When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/query/enable_logging`.
                public var enable_logging: Swift.Bool?
                /// You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                /// 0 - default mode (no latency optimizations)
                /// 1 - normal latency optimizations (about 50% of possible latency improvement of option 3)
                /// 2 - strong latency optimizations (about 75% of possible latency improvement of option 3)
                /// 3 - max latency optimizations
                /// 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
                ///
                /// Defaults to None.
                ///
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/query/optimize_streaming_latency`.
                @available(*, deprecated)
                public var optimize_streaming_latency: Swift.Int?
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/query/output_format`.
                @frozen public enum output_formatPayload: String, Codable, Hashable, Sendable, CaseIterable {
                    case mp3_22050_32 = "mp3_22050_32"
                    case mp3_24000_48 = "mp3_24000_48"
                    case mp3_44100_32 = "mp3_44100_32"
                    case mp3_44100_64 = "mp3_44100_64"
                    case mp3_44100_96 = "mp3_44100_96"
                    case mp3_44100_128 = "mp3_44100_128"
                    case mp3_44100_192 = "mp3_44100_192"
                    case pcm_8000 = "pcm_8000"
                    case pcm_16000 = "pcm_16000"
                    case pcm_22050 = "pcm_22050"
                    case pcm_24000 = "pcm_24000"
                    case pcm_32000 = "pcm_32000"
                    case pcm_44100 = "pcm_44100"
                    case pcm_48000 = "pcm_48000"
                    case ulaw_8000 = "ulaw_8000"
                    case alaw_8000 = "alaw_8000"
                    case opus_48000_32 = "opus_48000_32"
                    case opus_48000_64 = "opus_48000_64"
                    case opus_48000_96 = "opus_48000_96"
                    case opus_48000_128 = "opus_48000_128"
                    case opus_48000_192 = "opus_48000_192"
                }
                /// Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/query/output_format`.
                public var output_format: Operations.text_to_speech_stream_with_timestamps.Input.Query.output_formatPayload?
                /// Creates a new `Query`.
                ///
                /// - Parameters:
                ///   - enable_logging: When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
                ///   - optimize_streaming_latency: You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
                ///   - output_format: Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the -law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
                public init(
                    enable_logging: Swift.Bool? = nil,
                    optimize_streaming_latency: Swift.Int? = nil,
                    output_format: Operations.text_to_speech_stream_with_timestamps.Input.Query.output_formatPayload? = nil
                ) {
                    self.enable_logging = enable_logging
                    self.optimize_streaming_latency = optimize_streaming_latency
                    self.output_format = output_format
                }
            }
            public var query: Operations.text_to_speech_stream_with_timestamps.Input.Query
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/header`.
            public struct Headers: Sendable, Hashable {
                /// Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/header/xi-api-key`.
                public var xi_hyphen_api_hyphen_key: Swift.String?
                public var accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_stream_with_timestamps.AcceptableContentType>]
                /// Creates a new `Headers`.
                ///
                /// - Parameters:
                ///   - xi_hyphen_api_hyphen_key: Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///   - accept:
                public init(
                    xi_hyphen_api_hyphen_key: Swift.String? = nil,
                    accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.text_to_speech_stream_with_timestamps.AcceptableContentType>] = .defaultValues()
                ) {
                    self.xi_hyphen_api_hyphen_key = xi_hyphen_api_hyphen_key
                    self.accept = accept
                }
            }
            public var headers: Operations.text_to_speech_stream_with_timestamps.Input.Headers
            /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/requestBody`.
            @frozen public enum Body: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/requestBody/content/application\/json`.
                case json(Components.Schemas.Body_text_to_speech_stream_with_timestamps)
            }
            public var body: Operations.text_to_speech_stream_with_timestamps.Input.Body
            /// Creates a new `Input`.
            ///
            /// - Parameters:
            ///   - path:
            ///   - query:
            ///   - headers:
            ///   - body:
            public init(
                path: Operations.text_to_speech_stream_with_timestamps.Input.Path,
                query: Operations.text_to_speech_stream_with_timestamps.Input.Query = .init(),
                headers: Operations.text_to_speech_stream_with_timestamps.Input.Headers = .init(),
                body: Operations.text_to_speech_stream_with_timestamps.Input.Body
            ) {
                self.path = path
                self.query = query
                self.headers = headers
                self.body = body
            }
        }
        @frozen public enum Output: Sendable, Hashable {
            public struct Ok: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/responses/200/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/responses/200/content/application\/json`.
                    case json(Components.Schemas.StreamingAudioChunkWithTimestampsResponseModel)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.StreamingAudioChunkWithTimestampsResponseModel {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_stream_with_timestamps.Output.Ok.Body
                /// Creates a new `Ok`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_stream_with_timestamps.Output.Ok.Body) {
                    self.body = body
                }
            }
            /// Stream of transcription chunks
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/with-timestamps/post(text_to_speech_stream_with_timestamps)/responses/200`.
            ///
            /// HTTP response code: `200 ok`.
            case ok(Operations.text_to_speech_stream_with_timestamps.Output.Ok)
            /// The associated value of the enum case if `self` is `.ok`.
            ///
            /// - Throws: An error if `self` is not `.ok`.
            /// - SeeAlso: `.ok`.
            public var ok: Operations.text_to_speech_stream_with_timestamps.Output.Ok {
                get throws {
                    switch self {
                    case let .ok(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "ok",
                            response: self
                        )
                    }
                }
            }
            /// Bad Request - The request was invalid or malformed.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/with-timestamps/post(text_to_speech_stream_with_timestamps)/responses/400`.
            ///
            /// HTTP response code: `400 badRequest`.
            case badRequest(Components.Responses.BadRequest)
            /// The associated value of the enum case if `self` is `.badRequest`.
            ///
            /// - Throws: An error if `self` is not `.badRequest`.
            /// - SeeAlso: `.badRequest`.
            public var badRequest: Components.Responses.BadRequest {
                get throws {
                    switch self {
                    case let .badRequest(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "badRequest",
                            response: self
                        )
                    }
                }
            }
            /// Payment Required - A paid plan is required to access this resource.
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/with-timestamps/post(text_to_speech_stream_with_timestamps)/responses/402`.
            ///
            /// HTTP response code: `402 code402`.
            case code402(Components.Responses.PaymentRequired)
            /// The associated value of the enum case if `self` is `.code402`.
            ///
            /// - Throws: An error if `self` is not `.code402`.
            /// - SeeAlso: `.code402`.
            public var code402: Components.Responses.PaymentRequired {
                get throws {
                    switch self {
                    case let .code402(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "code402",
                            response: self
                        )
                    }
                }
            }
            public struct UnprocessableContent: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/responses/422/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/text-to-speech/{voice_id}/stream/with-timestamps/POST/responses/422/content/application\/json`.
                    case json(Components.Schemas.HTTPValidationError)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.HTTPValidationError {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.text_to_speech_stream_with_timestamps.Output.UnprocessableContent.Body
                /// Creates a new `UnprocessableContent`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.text_to_speech_stream_with_timestamps.Output.UnprocessableContent.Body) {
                    self.body = body
                }
            }
            /// Validation Error
            ///
            /// - Remark: Generated from `#/paths//v1/text-to-speech/{voice_id}/stream/with-timestamps/post(text_to_speech_stream_with_timestamps)/responses/422`.
            ///
            /// HTTP response code: `422 unprocessableContent`.
            case unprocessableContent(Operations.text_to_speech_stream_with_timestamps.Output.UnprocessableContent)
            /// The associated value of the enum case if `self` is `.unprocessableContent`.
            ///
            /// - Throws: An error if `self` is not `.unprocessableContent`.
            /// - SeeAlso: `.unprocessableContent`.
            public var unprocessableContent: Operations.text_to_speech_stream_with_timestamps.Output.UnprocessableContent {
                get throws {
                    switch self {
                    case let .unprocessableContent(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unprocessableContent",
                            response: self
                        )
                    }
                }
            }
            /// Undocumented response.
            ///
            /// A response with a code that is not documented in the OpenAPI document.
            case undocumented(statusCode: Swift.Int, OpenAPIRuntime.UndocumentedPayload)
        }
        @frozen public enum AcceptableContentType: AcceptableProtocol {
            case json
            case other(Swift.String)
            public init?(rawValue: Swift.String) {
                switch rawValue.lowercased() {
                case "application/json":
                    self = .json
                default:
                    self = .other(rawValue)
                }
            }
            public var rawValue: Swift.String {
                switch self {
                case let .other(string):
                    return string
                case .json:
                    return "application/json"
                }
            }
            public static var allCases: [Self] {
                [
                    .json
                ]
            }
        }
    }
    /// Speech To Text
    ///
    /// Transcribe an audio or video file. If webhook is set to true, the request will be processed asynchronously and results sent to configured webhooks. When use_multi_channel is true and the provided audio has multiple channels, a 'transcripts' object with separate transcripts for each channel is returned. Otherwise, returns a single transcript. The optional webhook_metadata parameter allows you to attach custom data that will be included in webhook responses for request correlation and tracking.
    ///
    /// - Remark: HTTP `POST /v1/speech-to-text`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)`.
    public enum speech_to_text {
        public static let id: Swift.String = "speech_to_text"
        public struct Input: Sendable, Hashable {
            /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/query`.
            public struct Query: Sendable, Hashable {
                /// When enable_logging is set to false zero retention mode will be used for the request. This will mean log and transcript storage features are unavailable for this request. Zero retention mode may only be used by enterprise customers.
                ///
                /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/query/enable_logging`.
                public var enable_logging: Swift.Bool?
                /// Creates a new `Query`.
                ///
                /// - Parameters:
                ///   - enable_logging: When enable_logging is set to false zero retention mode will be used for the request. This will mean log and transcript storage features are unavailable for this request. Zero retention mode may only be used by enterprise customers.
                public init(enable_logging: Swift.Bool? = nil) {
                    self.enable_logging = enable_logging
                }
            }
            public var query: Operations.speech_to_text.Input.Query
            /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/header`.
            public struct Headers: Sendable, Hashable {
                /// Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///
                /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/header/xi-api-key`.
                public var xi_hyphen_api_hyphen_key: Swift.String?
                public var accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.speech_to_text.AcceptableContentType>]
                /// Creates a new `Headers`.
                ///
                /// - Parameters:
                ///   - xi_hyphen_api_hyphen_key: Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///   - accept:
                public init(
                    xi_hyphen_api_hyphen_key: Swift.String? = nil,
                    accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.speech_to_text.AcceptableContentType>] = .defaultValues()
                ) {
                    self.xi_hyphen_api_hyphen_key = xi_hyphen_api_hyphen_key
                    self.accept = accept
                }
            }
            public var headers: Operations.speech_to_text.Input.Headers
            /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/requestBody`.
            @frozen public enum Body: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/requestBody/content/multipart\/form-data`.
                case multipartForm(OpenAPIRuntime.MultipartBody<Components.Schemas.Body_Speech_to_Text_v1_speech_to_text_post>)
            }
            public var body: Operations.speech_to_text.Input.Body
            /// Creates a new `Input`.
            ///
            /// - Parameters:
            ///   - query:
            ///   - headers:
            ///   - body:
            public init(
                query: Operations.speech_to_text.Input.Query = .init(),
                headers: Operations.speech_to_text.Input.Headers = .init(),
                body: Operations.speech_to_text.Input.Body
            ) {
                self.query = query
                self.headers = headers
                self.body = body
            }
        }
        @frozen public enum Output: Sendable, Hashable {
            public struct Ok: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/200/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/200/content/json`.
                    public struct jsonPayload: Codable, Hashable, Sendable {
                        /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/200/content/json/value1`.
                        public var value1: Components.Schemas.SpeechToTextChunkResponseModel?
                        /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/200/content/json/value2`.
                        public var value2: Components.Schemas.MultichannelSpeechToTextResponseModel?
                        /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/200/content/json/value3`.
                        public var value3: Components.Schemas.SpeechToTextWebhookResponseModel?
                        /// Creates a new `jsonPayload`.
                        ///
                        /// - Parameters:
                        ///   - value1:
                        ///   - value2:
                        ///   - value3:
                        public init(
                            value1: Components.Schemas.SpeechToTextChunkResponseModel? = nil,
                            value2: Components.Schemas.MultichannelSpeechToTextResponseModel? = nil,
                            value3: Components.Schemas.SpeechToTextWebhookResponseModel? = nil
                        ) {
                            self.value1 = value1
                            self.value2 = value2
                            self.value3 = value3
                        }
                        public init(from decoder: any Decoder) throws {
                            var errors: [any Error] = []
                            do {
                                self.value1 = try .init(from: decoder)
                            } catch {
                                errors.append(error)
                            }
                            do {
                                self.value2 = try .init(from: decoder)
                            } catch {
                                errors.append(error)
                            }
                            do {
                                self.value3 = try .init(from: decoder)
                            } catch {
                                errors.append(error)
                            }
                            try Swift.DecodingError.verifyAtLeastOneSchemaIsNotNil(
                                [
                                    self.value1,
                                    self.value2,
                                    self.value3
                                ],
                                type: Self.self,
                                codingPath: decoder.codingPath,
                                errors: errors
                            )
                        }
                        public func encode(to encoder: any Encoder) throws {
                            try self.value1?.encode(to: encoder)
                            try self.value2?.encode(to: encoder)
                            try self.value3?.encode(to: encoder)
                        }
                    }
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/200/content/application\/json`.
                    case json(Operations.speech_to_text.Output.Ok.Body.jsonPayload)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Operations.speech_to_text.Output.Ok.Body.jsonPayload {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.speech_to_text.Output.Ok.Body
                /// Creates a new `Ok`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.speech_to_text.Output.Ok.Body) {
                    self.body = body
                }
            }
            /// Synchronous transcription result
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)/responses/200`.
            ///
            /// HTTP response code: `200 ok`.
            case ok(Operations.speech_to_text.Output.Ok)
            /// The associated value of the enum case if `self` is `.ok`.
            ///
            /// - Throws: An error if `self` is not `.ok`.
            /// - SeeAlso: `.ok`.
            public var ok: Operations.speech_to_text.Output.Ok {
                get throws {
                    switch self {
                    case let .ok(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "ok",
                            response: self
                        )
                    }
                }
            }
            public struct Accepted: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/202/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/202/content/application\/json`.
                    case json(OpenAPIRuntime.OpenAPIValueContainer)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: OpenAPIRuntime.OpenAPIValueContainer {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.speech_to_text.Output.Accepted.Body
                /// Creates a new `Accepted`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.speech_to_text.Output.Accepted.Body) {
                    self.body = body
                }
            }
            /// Asynchronous request accepted
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)/responses/202`.
            ///
            /// HTTP response code: `202 accepted`.
            case accepted(Operations.speech_to_text.Output.Accepted)
            /// The associated value of the enum case if `self` is `.accepted`.
            ///
            /// - Throws: An error if `self` is not `.accepted`.
            /// - SeeAlso: `.accepted`.
            public var accepted: Operations.speech_to_text.Output.Accepted {
                get throws {
                    switch self {
                    case let .accepted(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "accepted",
                            response: self
                        )
                    }
                }
            }
            /// Bad Request - The request was invalid or malformed.
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)/responses/400`.
            ///
            /// HTTP response code: `400 badRequest`.
            case badRequest(Components.Responses.BadRequest)
            /// The associated value of the enum case if `self` is `.badRequest`.
            ///
            /// - Throws: An error if `self` is not `.badRequest`.
            /// - SeeAlso: `.badRequest`.
            public var badRequest: Components.Responses.BadRequest {
                get throws {
                    switch self {
                    case let .badRequest(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "badRequest",
                            response: self
                        )
                    }
                }
            }
            /// Payment Required - A paid plan is required to access this resource.
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)/responses/402`.
            ///
            /// HTTP response code: `402 code402`.
            case code402(Components.Responses.PaymentRequired)
            /// The associated value of the enum case if `self` is `.code402`.
            ///
            /// - Throws: An error if `self` is not `.code402`.
            /// - SeeAlso: `.code402`.
            public var code402: Components.Responses.PaymentRequired {
                get throws {
                    switch self {
                    case let .code402(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "code402",
                            response: self
                        )
                    }
                }
            }
            public struct UnprocessableContent: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/422/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/POST/responses/422/content/application\/json`.
                    case json(Components.Schemas.HTTPValidationError)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.HTTPValidationError {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.speech_to_text.Output.UnprocessableContent.Body
                /// Creates a new `UnprocessableContent`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.speech_to_text.Output.UnprocessableContent.Body) {
                    self.body = body
                }
            }
            /// Validation Error
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/post(speech_to_text)/responses/422`.
            ///
            /// HTTP response code: `422 unprocessableContent`.
            case unprocessableContent(Operations.speech_to_text.Output.UnprocessableContent)
            /// The associated value of the enum case if `self` is `.unprocessableContent`.
            ///
            /// - Throws: An error if `self` is not `.unprocessableContent`.
            /// - SeeAlso: `.unprocessableContent`.
            public var unprocessableContent: Operations.speech_to_text.Output.UnprocessableContent {
                get throws {
                    switch self {
                    case let .unprocessableContent(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unprocessableContent",
                            response: self
                        )
                    }
                }
            }
            /// Undocumented response.
            ///
            /// A response with a code that is not documented in the OpenAPI document.
            case undocumented(statusCode: Swift.Int, OpenAPIRuntime.UndocumentedPayload)
        }
        @frozen public enum AcceptableContentType: AcceptableProtocol {
            case json
            case other(Swift.String)
            public init?(rawValue: Swift.String) {
                switch rawValue.lowercased() {
                case "application/json":
                    self = .json
                default:
                    self = .other(rawValue)
                }
            }
            public var rawValue: Swift.String {
                switch self {
                case let .other(string):
                    return string
                case .json:
                    return "application/json"
                }
            }
            public static var allCases: [Self] {
                [
                    .json
                ]
            }
        }
    }
    /// Get Transcript By Id
    ///
    /// Retrieve a previously generated transcript by its ID.
    ///
    /// - Remark: HTTP `GET /v1/speech-to-text/transcripts/{transcription_id}`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)`.
    public enum get_transcript_by_id {
        public static let id: Swift.String = "get_transcript_by_id"
        public struct Input: Sendable, Hashable {
            /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/path`.
            public struct Path: Sendable, Hashable {
                /// The unique ID of the transcript to retrieve
                ///
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/path/transcription_id`.
                public var transcription_id: Swift.String
                /// Creates a new `Path`.
                ///
                /// - Parameters:
                ///   - transcription_id: The unique ID of the transcript to retrieve
                public init(transcription_id: Swift.String) {
                    self.transcription_id = transcription_id
                }
            }
            public var path: Operations.get_transcript_by_id.Input.Path
            /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/header`.
            public struct Headers: Sendable, Hashable {
                /// Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/header/xi-api-key`.
                public var xi_hyphen_api_hyphen_key: Swift.String?
                public var accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.get_transcript_by_id.AcceptableContentType>]
                /// Creates a new `Headers`.
                ///
                /// - Parameters:
                ///   - xi_hyphen_api_hyphen_key: Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///   - accept:
                public init(
                    xi_hyphen_api_hyphen_key: Swift.String? = nil,
                    accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.get_transcript_by_id.AcceptableContentType>] = .defaultValues()
                ) {
                    self.xi_hyphen_api_hyphen_key = xi_hyphen_api_hyphen_key
                    self.accept = accept
                }
            }
            public var headers: Operations.get_transcript_by_id.Input.Headers
            /// Creates a new `Input`.
            ///
            /// - Parameters:
            ///   - path:
            ///   - headers:
            public init(
                path: Operations.get_transcript_by_id.Input.Path,
                headers: Operations.get_transcript_by_id.Input.Headers = .init()
            ) {
                self.path = path
                self.headers = headers
            }
        }
        @frozen public enum Output: Sendable, Hashable {
            public struct Ok: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/200/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/200/content/json`.
                    public struct jsonPayload: Codable, Hashable, Sendable {
                        /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/200/content/json/value1`.
                        public var value1: Components.Schemas.SpeechToTextChunkResponseModel?
                        /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/200/content/json/value2`.
                        public var value2: Components.Schemas.MultichannelSpeechToTextResponseModel?
                        /// Creates a new `jsonPayload`.
                        ///
                        /// - Parameters:
                        ///   - value1:
                        ///   - value2:
                        public init(
                            value1: Components.Schemas.SpeechToTextChunkResponseModel? = nil,
                            value2: Components.Schemas.MultichannelSpeechToTextResponseModel? = nil
                        ) {
                            self.value1 = value1
                            self.value2 = value2
                        }
                        public init(from decoder: any Decoder) throws {
                            var errors: [any Error] = []
                            do {
                                self.value1 = try .init(from: decoder)
                            } catch {
                                errors.append(error)
                            }
                            do {
                                self.value2 = try .init(from: decoder)
                            } catch {
                                errors.append(error)
                            }
                            try Swift.DecodingError.verifyAtLeastOneSchemaIsNotNil(
                                [
                                    self.value1,
                                    self.value2
                                ],
                                type: Self.self,
                                codingPath: decoder.codingPath,
                                errors: errors
                            )
                        }
                        public func encode(to encoder: any Encoder) throws {
                            try self.value1?.encode(to: encoder)
                            try self.value2?.encode(to: encoder)
                        }
                    }
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/200/content/application\/json`.
                    case json(Operations.get_transcript_by_id.Output.Ok.Body.jsonPayload)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Operations.get_transcript_by_id.Output.Ok.Body.jsonPayload {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.get_transcript_by_id.Output.Ok.Body
                /// Creates a new `Ok`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.get_transcript_by_id.Output.Ok.Body) {
                    self.body = body
                }
            }
            /// The transcript data
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)/responses/200`.
            ///
            /// HTTP response code: `200 ok`.
            case ok(Operations.get_transcript_by_id.Output.Ok)
            /// The associated value of the enum case if `self` is `.ok`.
            ///
            /// - Throws: An error if `self` is not `.ok`.
            /// - SeeAlso: `.ok`.
            public var ok: Operations.get_transcript_by_id.Output.Ok {
                get throws {
                    switch self {
                    case let .ok(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "ok",
                            response: self
                        )
                    }
                }
            }
            /// Bad Request - The request was invalid or malformed.
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)/responses/400`.
            ///
            /// HTTP response code: `400 badRequest`.
            case badRequest(Components.Responses.BadRequest)
            /// The associated value of the enum case if `self` is `.badRequest`.
            ///
            /// - Throws: An error if `self` is not `.badRequest`.
            /// - SeeAlso: `.badRequest`.
            public var badRequest: Components.Responses.BadRequest {
                get throws {
                    switch self {
                    case let .badRequest(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "badRequest",
                            response: self
                        )
                    }
                }
            }
            public struct Unauthorized: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/401/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/401/content/application\/json`.
                    case json(OpenAPIRuntime.OpenAPIValueContainer)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: OpenAPIRuntime.OpenAPIValueContainer {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.get_transcript_by_id.Output.Unauthorized.Body
                /// Creates a new `Unauthorized`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.get_transcript_by_id.Output.Unauthorized.Body) {
                    self.body = body
                }
            }
            /// Authentication required
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)/responses/401`.
            ///
            /// HTTP response code: `401 unauthorized`.
            case unauthorized(Operations.get_transcript_by_id.Output.Unauthorized)
            /// The associated value of the enum case if `self` is `.unauthorized`.
            ///
            /// - Throws: An error if `self` is not `.unauthorized`.
            /// - SeeAlso: `.unauthorized`.
            public var unauthorized: Operations.get_transcript_by_id.Output.Unauthorized {
                get throws {
                    switch self {
                    case let .unauthorized(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unauthorized",
                            response: self
                        )
                    }
                }
            }
            /// Payment Required - A paid plan is required to access this resource.
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)/responses/402`.
            ///
            /// HTTP response code: `402 code402`.
            case code402(Components.Responses.PaymentRequired)
            /// The associated value of the enum case if `self` is `.code402`.
            ///
            /// - Throws: An error if `self` is not `.code402`.
            /// - SeeAlso: `.code402`.
            public var code402: Components.Responses.PaymentRequired {
                get throws {
                    switch self {
                    case let .code402(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "code402",
                            response: self
                        )
                    }
                }
            }
            public struct NotFound: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/404/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/404/content/application\/json`.
                    case json(OpenAPIRuntime.OpenAPIValueContainer)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: OpenAPIRuntime.OpenAPIValueContainer {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.get_transcript_by_id.Output.NotFound.Body
                /// Creates a new `NotFound`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.get_transcript_by_id.Output.NotFound.Body) {
                    self.body = body
                }
            }
            /// Transcript not found
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)/responses/404`.
            ///
            /// HTTP response code: `404 notFound`.
            case notFound(Operations.get_transcript_by_id.Output.NotFound)
            /// The associated value of the enum case if `self` is `.notFound`.
            ///
            /// - Throws: An error if `self` is not `.notFound`.
            /// - SeeAlso: `.notFound`.
            public var notFound: Operations.get_transcript_by_id.Output.NotFound {
                get throws {
                    switch self {
                    case let .notFound(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "notFound",
                            response: self
                        )
                    }
                }
            }
            public struct UnprocessableContent: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/422/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/GET/responses/422/content/application\/json`.
                    case json(Components.Schemas.HTTPValidationError)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.HTTPValidationError {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.get_transcript_by_id.Output.UnprocessableContent.Body
                /// Creates a new `UnprocessableContent`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.get_transcript_by_id.Output.UnprocessableContent.Body) {
                    self.body = body
                }
            }
            /// Validation Error
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/get(get_transcript_by_id)/responses/422`.
            ///
            /// HTTP response code: `422 unprocessableContent`.
            case unprocessableContent(Operations.get_transcript_by_id.Output.UnprocessableContent)
            /// The associated value of the enum case if `self` is `.unprocessableContent`.
            ///
            /// - Throws: An error if `self` is not `.unprocessableContent`.
            /// - SeeAlso: `.unprocessableContent`.
            public var unprocessableContent: Operations.get_transcript_by_id.Output.UnprocessableContent {
                get throws {
                    switch self {
                    case let .unprocessableContent(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unprocessableContent",
                            response: self
                        )
                    }
                }
            }
            /// Undocumented response.
            ///
            /// A response with a code that is not documented in the OpenAPI document.
            case undocumented(statusCode: Swift.Int, OpenAPIRuntime.UndocumentedPayload)
        }
        @frozen public enum AcceptableContentType: AcceptableProtocol {
            case json
            case other(Swift.String)
            public init?(rawValue: Swift.String) {
                switch rawValue.lowercased() {
                case "application/json":
                    self = .json
                default:
                    self = .other(rawValue)
                }
            }
            public var rawValue: Swift.String {
                switch self {
                case let .other(string):
                    return string
                case .json:
                    return "application/json"
                }
            }
            public static var allCases: [Self] {
                [
                    .json
                ]
            }
        }
    }
    /// Delete Transcript By Id
    ///
    /// Delete a previously generated transcript by its ID.
    ///
    /// - Remark: HTTP `DELETE /v1/speech-to-text/transcripts/{transcription_id}`.
    /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)`.
    public enum delete_transcript_by_id {
        public static let id: Swift.String = "delete_transcript_by_id"
        public struct Input: Sendable, Hashable {
            /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/path`.
            public struct Path: Sendable, Hashable {
                /// The unique ID of the transcript to delete
                ///
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/path/transcription_id`.
                public var transcription_id: Swift.String
                /// Creates a new `Path`.
                ///
                /// - Parameters:
                ///   - transcription_id: The unique ID of the transcript to delete
                public init(transcription_id: Swift.String) {
                    self.transcription_id = transcription_id
                }
            }
            public var path: Operations.delete_transcript_by_id.Input.Path
            /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/header`.
            public struct Headers: Sendable, Hashable {
                /// Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/header/xi-api-key`.
                public var xi_hyphen_api_hyphen_key: Swift.String?
                public var accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.delete_transcript_by_id.AcceptableContentType>]
                /// Creates a new `Headers`.
                ///
                /// - Parameters:
                ///   - xi_hyphen_api_hyphen_key: Your API key. This is required by most endpoints to access our API programmatically. You can view your xi-api-key using the 'Profile' tab on the website.
                ///   - accept:
                public init(
                    xi_hyphen_api_hyphen_key: Swift.String? = nil,
                    accept: [OpenAPIRuntime.AcceptHeaderContentType<Operations.delete_transcript_by_id.AcceptableContentType>] = .defaultValues()
                ) {
                    self.xi_hyphen_api_hyphen_key = xi_hyphen_api_hyphen_key
                    self.accept = accept
                }
            }
            public var headers: Operations.delete_transcript_by_id.Input.Headers
            /// Creates a new `Input`.
            ///
            /// - Parameters:
            ///   - path:
            ///   - headers:
            public init(
                path: Operations.delete_transcript_by_id.Input.Path,
                headers: Operations.delete_transcript_by_id.Input.Headers = .init()
            ) {
                self.path = path
                self.headers = headers
            }
        }
        @frozen public enum Output: Sendable, Hashable {
            public struct Ok: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/responses/200/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/responses/200/content/application\/json`.
                    case json(OpenAPIRuntime.OpenAPIValueContainer)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: OpenAPIRuntime.OpenAPIValueContainer {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.delete_transcript_by_id.Output.Ok.Body
                /// Creates a new `Ok`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.delete_transcript_by_id.Output.Ok.Body) {
                    self.body = body
                }
            }
            /// Delete completed successfully.
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)/responses/200`.
            ///
            /// HTTP response code: `200 ok`.
            case ok(Operations.delete_transcript_by_id.Output.Ok)
            /// The associated value of the enum case if `self` is `.ok`.
            ///
            /// - Throws: An error if `self` is not `.ok`.
            /// - SeeAlso: `.ok`.
            public var ok: Operations.delete_transcript_by_id.Output.Ok {
                get throws {
                    switch self {
                    case let .ok(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "ok",
                            response: self
                        )
                    }
                }
            }
            /// Bad Request - The request was invalid or malformed.
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)/responses/400`.
            ///
            /// HTTP response code: `400 badRequest`.
            case badRequest(Components.Responses.BadRequest)
            /// The associated value of the enum case if `self` is `.badRequest`.
            ///
            /// - Throws: An error if `self` is not `.badRequest`.
            /// - SeeAlso: `.badRequest`.
            public var badRequest: Components.Responses.BadRequest {
                get throws {
                    switch self {
                    case let .badRequest(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "badRequest",
                            response: self
                        )
                    }
                }
            }
            public struct Unauthorized: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/responses/401/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/responses/401/content/application\/json`.
                    case json(OpenAPIRuntime.OpenAPIValueContainer)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: OpenAPIRuntime.OpenAPIValueContainer {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.delete_transcript_by_id.Output.Unauthorized.Body
                /// Creates a new `Unauthorized`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.delete_transcript_by_id.Output.Unauthorized.Body) {
                    self.body = body
                }
            }
            /// Authentication required
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)/responses/401`.
            ///
            /// HTTP response code: `401 unauthorized`.
            case unauthorized(Operations.delete_transcript_by_id.Output.Unauthorized)
            /// The associated value of the enum case if `self` is `.unauthorized`.
            ///
            /// - Throws: An error if `self` is not `.unauthorized`.
            /// - SeeAlso: `.unauthorized`.
            public var unauthorized: Operations.delete_transcript_by_id.Output.Unauthorized {
                get throws {
                    switch self {
                    case let .unauthorized(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unauthorized",
                            response: self
                        )
                    }
                }
            }
            /// Payment Required - A paid plan is required to access this resource.
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)/responses/402`.
            ///
            /// HTTP response code: `402 code402`.
            case code402(Components.Responses.PaymentRequired)
            /// The associated value of the enum case if `self` is `.code402`.
            ///
            /// - Throws: An error if `self` is not `.code402`.
            /// - SeeAlso: `.code402`.
            public var code402: Components.Responses.PaymentRequired {
                get throws {
                    switch self {
                    case let .code402(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "code402",
                            response: self
                        )
                    }
                }
            }
            public struct UnprocessableContent: Sendable, Hashable {
                /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/responses/422/content`.
                @frozen public enum Body: Sendable, Hashable {
                    /// - Remark: Generated from `#/paths/v1/speech-to-text/transcripts/{transcription_id}/DELETE/responses/422/content/application\/json`.
                    case json(Components.Schemas.HTTPValidationError)
                    /// The associated value of the enum case if `self` is `.json`.
                    ///
                    /// - Throws: An error if `self` is not `.json`.
                    /// - SeeAlso: `.json`.
                    public var json: Components.Schemas.HTTPValidationError {
                        get throws {
                            switch self {
                            case let .json(body):
                                return body
                            }
                        }
                    }
                }
                /// Received HTTP response body
                public var body: Operations.delete_transcript_by_id.Output.UnprocessableContent.Body
                /// Creates a new `UnprocessableContent`.
                ///
                /// - Parameters:
                ///   - body: Received HTTP response body
                public init(body: Operations.delete_transcript_by_id.Output.UnprocessableContent.Body) {
                    self.body = body
                }
            }
            /// Validation Error
            ///
            /// - Remark: Generated from `#/paths//v1/speech-to-text/transcripts/{transcription_id}/delete(delete_transcript_by_id)/responses/422`.
            ///
            /// HTTP response code: `422 unprocessableContent`.
            case unprocessableContent(Operations.delete_transcript_by_id.Output.UnprocessableContent)
            /// The associated value of the enum case if `self` is `.unprocessableContent`.
            ///
            /// - Throws: An error if `self` is not `.unprocessableContent`.
            /// - SeeAlso: `.unprocessableContent`.
            public var unprocessableContent: Operations.delete_transcript_by_id.Output.UnprocessableContent {
                get throws {
                    switch self {
                    case let .unprocessableContent(response):
                        return response
                    default:
                        try throwUnexpectedResponseStatus(
                            expectedStatus: "unprocessableContent",
                            response: self
                        )
                    }
                }
            }
            /// Undocumented response.
            ///
            /// A response with a code that is not documented in the OpenAPI document.
            case undocumented(statusCode: Swift.Int, OpenAPIRuntime.UndocumentedPayload)
        }
        @frozen public enum AcceptableContentType: AcceptableProtocol {
            case json
            case other(Swift.String)
            public init?(rawValue: Swift.String) {
                switch rawValue.lowercased() {
                case "application/json":
                    self = .json
                default:
                    self = .other(rawValue)
                }
            }
            public var rawValue: Swift.String {
                switch self {
                case let .other(string):
                    return string
                case .json:
                    return "application/json"
                }
            }
            public static var allCases: [Self] {
                [
                    .json
                ]
            }
        }
    }
}
